{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "UNIK.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7QP48bAaMgR"
      },
      "source": [
        "import math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.autograd import Variable\n",
        "from sklearn.utils import shuffle"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s86-bOvUbxCU"
      },
      "source": [
        "def import_class(name):\n",
        "    components = name.split('.')\n",
        "    mod = __import__(components[0])\n",
        "    for comp in components[1:]:\n",
        "        mod = getattr(mod, comp)\n",
        "    return mod\n",
        "\n",
        "\n",
        "def conv_branch_init(conv, branches):\n",
        "    weight = conv.weight\n",
        "    n = weight.size(0)\n",
        "    k1 = weight.size(1)\n",
        "    k2 = weight.size(2)\n",
        "    nn.init.normal_(weight, 0, math.sqrt(2. / (n * k1 * k2 * branches)))\n",
        "    nn.init.constant_(conv.bias, 0)\n",
        "\n",
        "\n",
        "def conv_init(conv):\n",
        "    nn.init.kaiming_normal_(conv.weight, mode='fan_out')\n",
        "    nn.init.constant_(conv.bias, 0)\n",
        "\n",
        "\n",
        "def bn_init(bn, scale):\n",
        "    nn.init.constant_(bn.weight, scale)\n",
        "    nn.init.constant_(bn.bias, 0)\n",
        "\n",
        "class UnfoldTemporalWindows(nn.Module):\n",
        "    def __init__(self, window_size, window_stride=1, window_dilation=1):\n",
        "        super().__init__()\n",
        "        self.window_size = window_size\n",
        "        self.window_stride = window_stride\n",
        "        self.window_dilation = window_dilation\n",
        "\n",
        "        self.padding = (window_size + (window_size-1) * (window_dilation-1) - 1) // 2\n",
        "        self.unfold = nn.Unfold(kernel_size=(self.window_size, 1),\n",
        "                                dilation=(self.window_dilation, 1),\n",
        "                                stride=(self.window_stride, 1),\n",
        "                                padding=(self.padding, 0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input shape: (N,C,T,V), out: (N,C,T,V*tau)\n",
        "        N, C, T, V = x.shape\n",
        "        x = self.unfold(x)\n",
        "        x = x.view(N, C, self.window_size, -1, V).permute(0,1,3,2,4).contiguous()\n",
        "        x = x.view(N, C, -1, self.window_size * V)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Temporal unit\n",
        "class T_LSU(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, dilation=1, autopad=True):\n",
        "        super(T_LSU, self).__init__()\n",
        "        if autopad:\n",
        "            pad = int(( kernel_size - 1) * dilation // 2)\n",
        "        else:\n",
        "            pad = 0\n",
        "\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=(kernel_size, 1), padding=(pad, 0),\n",
        "                              stride=(stride, 1), dilation=(dilation, 1))\n",
        "   \n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU()\n",
        "        conv_init(self.conv)\n",
        "        bn_init(self.bn, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.bn(self.conv(x))\n",
        "        return x\n",
        "\n",
        "# Spatial unit\n",
        "class S_LSU(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, num_joints, tau=1, num_heads=8, coff_embedding=4, bias=True):\n",
        "        super(S_LSU, self).__init__()\n",
        "        inter_channels = out_channels // coff_embedding\n",
        "        self.inter_c = inter_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.tau = tau\n",
        "        self.num_heads = num_heads\n",
        "        self.DepM = nn.Parameter(torch.Tensor(num_heads, num_joints*tau, num_joints*tau))\n",
        "        if bias:\n",
        "            self.bias = nn.Parameter(torch.Tensor(num_joints*tau))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "        self.reset_parameters()\n",
        "        \n",
        "        # Temporal window        \n",
        "        if tau != 1:\n",
        "            self.tw = UnfoldTemporalWindows(window_size=tau, window_stride=1, window_dilation=1)\n",
        "            self.out_conv = nn.Conv3d(out_channels, out_channels, kernel_size=(1, tau, 1))\n",
        "            self.out_bn = nn.BatchNorm2d(out_channels)\n",
        "        # Attention\n",
        "        self.conv_a = nn.ModuleList()\n",
        "        self.conv_b = nn.ModuleList()\n",
        "        self.conv_d = nn.ModuleList()\n",
        "        for i in range(self.num_heads):\n",
        "            self.conv_a.append(nn.Conv2d(in_channels, inter_channels, 1))\n",
        "            self.conv_b.append(nn.Conv2d(in_channels, inter_channels, 1))\n",
        "            self.conv_d.append(nn.Conv2d(in_channels, out_channels, 1))\n",
        "\n",
        "        if in_channels != out_channels:\n",
        "            self.down = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, 1),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "        else:\n",
        "            self.down = lambda x: x\n",
        "\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.soft = nn.Softmax(-2)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                conv_init(m)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                bn_init(m, 1)\n",
        "        bn_init(self.bn, 1e-6)\n",
        "        for i in range(self.num_heads):\n",
        "            conv_branch_init(self.conv_d[i], self.num_heads)\n",
        "            \n",
        "            \n",
        "    def reset_parameters(self) -> None:\n",
        "        nn.init.kaiming_uniform_(self.DepM, a=math.sqrt(5))\n",
        "        if self.bias is not None:\n",
        "            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.DepM)\n",
        "            bound = 1 / math.sqrt(fan_in)\n",
        "            nn.init.uniform_(self.bias, -bound, bound)\n",
        "            \n",
        "            \n",
        "    def forward(self, x):\n",
        "        if self.tau != 1:\n",
        "            x = self.tw(x)\n",
        "        N, C, T, V = x.size()\n",
        "     \n",
        "        W = self.DepM\n",
        "        B = self.bias\n",
        "        y = None\n",
        "        for i in range(self.num_heads):\n",
        "           \n",
        "            A1 = self.conv_a[i](x).permute(0, 3, 1, 2).contiguous().view(N, V, self.inter_c * T)\n",
        "            A2 = self.conv_b[i](x).view(N, self.inter_c * T, V)\n",
        "            A1 = self.soft(torch.matmul(A1, A2) / A1.size(-1))  # N tV tV\n",
        "            \n",
        "            A1 = W[i] + A1\n",
        "            A2 = x.view(N, C * T, V)\n",
        "            z = self.conv_d[i]((torch.matmul(A2, A1)).view(N, C, T, V))\n",
        "            y = z + y if y is not None else z\n",
        "\n",
        "        y = self.bn(y)\n",
        "        y += self.down(x)\n",
        "        \n",
        "        if self.tau == 1:\n",
        "            return self.relu(y).view(N, -1, T, V)\n",
        "        else:\n",
        "            y = self.relu(y)\n",
        "            y = y.view(N, self.out_channels, -1, self.tau, V // self.tau)\n",
        "            y = self.out_conv(y).squeeze(dim=3)\n",
        "            y = self.out_bn(y)\n",
        "            return y\n",
        "\n",
        "\n",
        "\n",
        "class ST_block(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, num_joints=20, tau=1, num_heads=3, stride=1, dilation=1, autopad=True, residual=True):\n",
        "        super(ST_block, self).__init__()\n",
        "        self.s_unit = S_LSU(in_channels, out_channels, num_joints, tau, num_heads)\n",
        "        self.t_unit = T_LSU(out_channels, out_channels, stride=stride, dilation=dilation, autopad=autopad)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        self.pad = 0\n",
        "        if not autopad:\n",
        "            self.pad = (9 - 1) * dilation // 2\n",
        "\n",
        "        if not residual:\n",
        "            self.residual = lambda x: 0\n",
        "\n",
        "        elif (in_channels == out_channels) and (stride == 1):\n",
        "            self.residual = lambda x: x\n",
        "\n",
        "        else:\n",
        "            self.residual = T_LSU(in_channels, out_channels, kernel_size=1, stride=stride)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.t_unit(self.s_unit(x)) + self.residual(x[:, :, self.pad : x.shape[2] - self.pad, :])\n",
        "        return self.relu(x)\n",
        "\n",
        "\n",
        "class UNIK(nn.Module):\n",
        "    def __init__(self, num_class=49, num_joints=20, num_person=1, tau=1, num_heads=3, in_channels=3):\n",
        "        super(UNIK, self).__init__()\n",
        "\n",
        "        \n",
        "        self.tau = tau\n",
        "        self.data_bn = nn.BatchNorm1d(num_person * in_channels * num_joints)\n",
        "\n",
        "        self.l1 = ST_block(in_channels, 64, num_joints, tau, residual=False)\n",
        "        \n",
        "        self.l2 = ST_block(64, 64, num_joints, tau, num_heads, dilation=1) #3\n",
        "        self.l3 = ST_block(64, 64, num_joints, tau, num_heads, dilation=1)  #3\n",
        "        self.l4 = ST_block(64, 64, num_joints, tau, num_heads, dilation=1)   #3\n",
        "        \n",
        "        self.l5 = ST_block(64, 128, num_joints, tau, num_heads, stride=2)\n",
        "        self.l6 = ST_block(128, 128, num_joints, tau, num_heads)\n",
        "        self.l7 = ST_block(128, 128, num_joints, tau, num_heads)\n",
        "        \n",
        "        self.l8 = ST_block(128, 256, num_joints, tau, num_heads, stride=2)\n",
        "        self.l9 = ST_block(256, 256, num_joints, tau, num_heads)\n",
        "        self.l10 = ST_block(256, 256, num_joints, tau, num_heads)\n",
        "\n",
        "        self.fc = nn.Linear(256, num_class)\n",
        "        nn.init.normal_(self.fc.weight, 0, math.sqrt(2. / num_class))\n",
        "        bn_init(self.data_bn, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        N, C, V, T = x.size()\n",
        "        x = x.permute(0,2,1,3).contiguous().view(N,V*C,T)\n",
        "        x = self.data_bn(x)\n",
        "        x = x.view(N,V,C,T).permute(0,2,3,1).contiguous().view(N,C,T,V)\n",
        "\n",
        "        x = self.l1(x)\n",
        "        x = self.l2(x)\n",
        "        x = self.l3(x)\n",
        "        x = self.l4(x)\n",
        "        x = self.l5(x)\n",
        "        x = self.l6(x)\n",
        "        x = self.l7(x)\n",
        "        x = self.l8(x)\n",
        "        x = self.l9(x)\n",
        "        x = self.l10(x)\n",
        "\n",
        "\n",
        "        c_new = x.size(1)\n",
        "        x=x.view(N,c_new,-1)\n",
        "        x = x.mean(2)\n",
        "\n",
        "        return self.fc(x)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BaphY4zubAp3"
      },
      "source": [
        "class GCNDataset(Dataset):\n",
        "    def __init__(self,filename,hasLabel=True):\n",
        "        self.df = pd.read_csv(filename,header=None)\n",
        "        self.length = len(self.df)\n",
        "        if hasLabel:\n",
        "            #self.df['freq']=1./self.df.groupby(961)[961].transform('count')\n",
        "            #self.df = self.df.sample(frac=1,weights=self.df.freq).reset_index(drop=True)\n",
        "            self.df = self.df.sample(frac=1).reset_index(drop=True)\n",
        "            self.X = torch.tensor(self.df.iloc[:,1:-1].values.astype('float32'))\n",
        "            # N, C, V, T\n",
        "            self.X = self.X.reshape((self.length ,16, 20, 3)).permute(0,3,2,1).contiguous()\n",
        "            labels = self.df.iloc[:,-1].values.astype('int32')-1\n",
        "            self.Y = np.zeros((labels.size, labels.max()+1))\n",
        "            self.Y[np.arange(labels.size),labels] = 1\n",
        "        else:\n",
        "            self.X = torch.tensor(self.df.iloc[:,1:].values.astype('float32'))\n",
        "            self.X = self.X.reshape((self.length ,16, 20, 3)).permute(0,3,2,1).contiguous()\n",
        "            self.Y = np.zeros((self.length,49))\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    \n",
        "    def __getitem__(self,index):\n",
        "        x = self.X[index]\n",
        "        y = self.Y[index]\n",
        "        return x, y "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9yV7b5ibLaE"
      },
      "source": [
        "train_set = GCNDataset(filename=\"drive/MyDrive/SML/training_set.csv\")\n",
        "val_set = GCNDataset(filename=\"drive/MyDrive/SML/val_set.csv\")\n",
        "test_set = GCNDataset(filename=\"drive/MyDrive/SML/test.csv\",hasLabel=False)\n",
        "train_loader = DataLoader(train_set,batch_size=64)\n",
        "val_loader = DataLoader(val_set,batch_size=64)\n",
        "test_loader = DataLoader(test_set,batch_size=64)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2fXqnK7c_Yc",
        "outputId": "37d2de95-b7a9-4e2e-de09-078e0f97bb1f"
      },
      "source": [
        "net = UNIK()\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "opti = optim.Adam(net.parameters(), lr = 2e-5)\n",
        "\n",
        "gpu = 0 #gpu ID\n",
        "net.cuda(gpu)\n",
        "print()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nkJPZ0n9ds9U",
        "outputId": "0e921a45-16a8-4222-b783-a28663c45422"
      },
      "source": [
        "def accuracy(logit,target):\n",
        "    a=(torch.argmax(logit,dim=1)==torch.argmax(target,dim=1)).sum()\n",
        "    return a\n",
        "\n",
        "def evaluate(model, criterion, dataloader, gpu):\n",
        "    model.eval()\n",
        "    acc = 0\n",
        "    count = 0\n",
        "    with torch.no_grad():\n",
        "        for i,(x,y) in enumerate(dataloader):\n",
        "            x,y = x.cuda(gpu), y.cuda(gpu)\n",
        "            logits = model(x)\n",
        "            acc+= accuracy(logits, y)\n",
        "            count += 64\n",
        "\n",
        "    return acc / count\n",
        "\n",
        "def train():\n",
        "    best_acc=0.35\n",
        "    best_epoch = 0\n",
        "    for epoch in range(200):\n",
        "        print(\"epoch:\",epoch)\n",
        "        total = 0\n",
        "        correct = 0\n",
        "        for i, (x,y) in enumerate(train_loader):\n",
        "            opti.zero_grad()\n",
        "            x,y=x.cuda(gpu),y.cuda(gpu)\n",
        "            opti.zero_grad()\n",
        "            logit = net(x)\n",
        "            loss = criterion(logit,y)\n",
        "            loss.backward()\n",
        "            opti.step()\n",
        "            correct+=accuracy(logit,y)\n",
        "            total+=64\n",
        "        print(\"train acc:\",correct/total)\n",
        "\n",
        "        dev_acc = evaluate(net, criterion, val_loader, gpu)\n",
        "        if dev_acc>best_acc:\n",
        "            best_acc=dev_acc\n",
        "            torch.save(net.state_dict(), 'stgcn-best.pkl')\n",
        "            torch.save(opti.state_dict(), \"stgcn_best_optim.pkl\")\n",
        "        print(\"dev_acc:\",dev_acc)\n",
        "train()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 0\n",
            "train acc: tensor(0.0083, device='cuda:0')\n",
            "dev_acc: tensor(0.0099, device='cuda:0')\n",
            "epoch: 1\n",
            "train acc: tensor(0.0983, device='cuda:0')\n",
            "dev_acc: tensor(0.1536, device='cuda:0')\n",
            "epoch: 2\n",
            "train acc: tensor(0.1838, device='cuda:0')\n",
            "dev_acc: tensor(0.1813, device='cuda:0')\n",
            "epoch: 3\n",
            "train acc: tensor(0.2121, device='cuda:0')\n",
            "dev_acc: tensor(0.2031, device='cuda:0')\n",
            "epoch: 4\n",
            "train acc: tensor(0.2321, device='cuda:0')\n",
            "dev_acc: tensor(0.2156, device='cuda:0')\n",
            "epoch: 5\n",
            "train acc: tensor(0.2448, device='cuda:0')\n",
            "dev_acc: tensor(0.2234, device='cuda:0')\n",
            "epoch: 6\n",
            "train acc: tensor(0.2554, device='cuda:0')\n",
            "dev_acc: tensor(0.2313, device='cuda:0')\n",
            "epoch: 7\n",
            "train acc: tensor(0.2646, device='cuda:0')\n",
            "dev_acc: tensor(0.2411, device='cuda:0')\n",
            "epoch: 8\n",
            "train acc: tensor(0.2728, device='cuda:0')\n",
            "dev_acc: tensor(0.2484, device='cuda:0')\n",
            "epoch: 9\n",
            "train acc: tensor(0.2835, device='cuda:0')\n",
            "dev_acc: tensor(0.2526, device='cuda:0')\n",
            "epoch: 10\n",
            "train acc: tensor(0.2930, device='cuda:0')\n",
            "dev_acc: tensor(0.2563, device='cuda:0')\n",
            "epoch: 11\n",
            "train acc: tensor(0.2989, device='cuda:0')\n",
            "dev_acc: tensor(0.2594, device='cuda:0')\n",
            "epoch: 12\n",
            "train acc: tensor(0.3065, device='cuda:0')\n",
            "dev_acc: tensor(0.2641, device='cuda:0')\n",
            "epoch: 13\n",
            "train acc: tensor(0.3121, device='cuda:0')\n",
            "dev_acc: tensor(0.2672, device='cuda:0')\n",
            "epoch: 14\n",
            "train acc: tensor(0.3195, device='cuda:0')\n",
            "dev_acc: tensor(0.2724, device='cuda:0')\n",
            "epoch: 15\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-80524b9346d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopti\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"stgcn_best_optim.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dev_acc:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdev_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-80524b9346d3>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mopti\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0mcorrect\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mtotal\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    116\u001b[0m                    \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                    \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m                    eps=group['eps'])\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sqs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}