{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "UNIK.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7QP48bAaMgR"
      },
      "source": [
        "import math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.utils import shuffle"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-QvW9GHkhUnN",
        "outputId": "c2409213-cf25-438b-cf04-a16974ab1be9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s86-bOvUbxCU"
      },
      "source": [
        "def conv_branch_init(conv, branches):\n",
        "    weight = conv.weight\n",
        "    n = weight.size(0)\n",
        "    k1 = weight.size(1)\n",
        "    k2 = weight.size(2)\n",
        "    nn.init.normal_(weight, 0, math.sqrt(2. / (n * k1 * k2 * branches)))\n",
        "    nn.init.constant_(conv.bias, 0)\n",
        "\n",
        "\n",
        "def conv_init(conv):\n",
        "    nn.init.kaiming_normal_(conv.weight, mode='fan_out')\n",
        "    nn.init.constant_(conv.bias, 0)\n",
        "\n",
        "\n",
        "def bn_init(bn, scale):\n",
        "    nn.init.constant_(bn.weight, scale)\n",
        "    nn.init.constant_(bn.bias, 0)\n",
        "\n",
        "\n",
        "# Temporal unit\n",
        "class T_LSU(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=9, stride=1, dilation=1, autopad=True):\n",
        "        super(T_LSU, self).__init__()\n",
        "\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU()\n",
        "        bn_init(self.bn, 1)\n",
        "        if autopad:\n",
        "            pad = int(( kernel_size - 1) * dilation // 2)\n",
        "        else:\n",
        "            pad = 0\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=(kernel_size, 1), padding=(pad, 0),\n",
        "                              stride=(stride, 1), dilation=(dilation, 1))\n",
        "        conv_init(self.conv)\n",
        "\n",
        "    def forward(self, x):\n",
        "        conv_part = self.bn(self.conv(x))\n",
        "        return conv_part\n",
        "\n",
        "# Spatial unit\n",
        "class S_LSU(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, num_joints, num_heads=8, coff_embedding=4, bias=True):\n",
        "        super(S_LSU, self).__init__()\n",
        "        inter_channels = out_channels // coff_embedding\n",
        "        self.inter_c = inter_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.num_heads = num_heads\n",
        "        self.DepM = nn.Parameter(torch.Tensor(num_heads, num_joints, num_joints))\n",
        "        if bias:\n",
        "            self.bias = nn.Parameter(torch.Tensor(num_joints))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "        self.reset_parameters()\n",
        "\n",
        "        # Attention\n",
        "        self.conv_a = nn.ModuleList()\n",
        "        self.conv_b = nn.ModuleList()\n",
        "        self.conv_d = nn.ModuleList()\n",
        "        for i in range(self.num_heads):\n",
        "            self.conv_a.append(nn.Conv2d(in_channels, inter_channels, 1))\n",
        "            self.conv_b.append(nn.Conv2d(in_channels, inter_channels, 1))\n",
        "            self.conv_d.append(nn.Conv2d(in_channels, out_channels, 1))\n",
        "\n",
        "        if in_channels != out_channels:\n",
        "            self.down = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, 1),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "        else:\n",
        "            self.down = lambda x: x\n",
        "\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.soft = nn.Softmax(-2)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                conv_init(m)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                bn_init(m, 1)\n",
        "        bn_init(self.bn, 1e-6)\n",
        "        for i in range(self.num_heads):\n",
        "            conv_branch_init(self.conv_d[i], self.num_heads)\n",
        "\n",
        "\n",
        "    def reset_parameters(self) -> None:\n",
        "        nn.init.kaiming_uniform_(self.DepM, a=math.sqrt(5))\n",
        "        if self.bias is not None:\n",
        "            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.DepM)\n",
        "            bound = 1 / math.sqrt(fan_in)\n",
        "            nn.init.uniform_(self.bias, -bound, bound)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        N, C, T, V = x.size()\n",
        "\n",
        "        W = self.DepM\n",
        "        B = self.bias\n",
        "        y = None\n",
        "        for i in range(self.num_heads):\n",
        "\n",
        "            A1 = self.conv_a[i](x).permute(0, 3, 1, 2).contiguous().view(N, V, self.inter_c * T)\n",
        "            A2 = self.conv_b[i](x).view(N, self.inter_c * T, V)\n",
        "            A1 = self.soft(torch.matmul(A1, A2) / A1.size(-1))  # N tV tV\n",
        "\n",
        "            A1 = W[i] + A1\n",
        "            A2 = x.view(N, C * T, V)\n",
        "            z = self.conv_d[i]((torch.matmul(A2, A1)).view(N, C, T, V))\n",
        "            y = z + y if y is not None else z\n",
        "\n",
        "        y = self.bn(y)\n",
        "        y += self.down(x)\n",
        "        return self.relu(y).view(N, -1, T, V)\n",
        "\n",
        "\n",
        "class ST_block(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, num_joints=25, num_heads=3, stride=1, dilation=1, autopad=True, residual=True):\n",
        "        super(ST_block, self).__init__()\n",
        "        self.s_unit = S_LSU(in_channels, out_channels, num_joints, num_heads)\n",
        "        self.t_unit = T_LSU(out_channels, out_channels, stride=stride, dilation=dilation, autopad=autopad)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.pad = 0\n",
        "        if not autopad:\n",
        "            self.pad = (9 - 1) * dilation // 2\n",
        "\n",
        "        if not residual:\n",
        "            self.residual = lambda x: 0\n",
        "\n",
        "        elif (in_channels == out_channels) and (stride == 1):\n",
        "            self.residual = lambda x: x\n",
        "\n",
        "        else:\n",
        "            self.residual = T_LSU(in_channels, out_channels, kernel_size=1, stride=stride)\n",
        "\n",
        "    def forward(self, x):\n",
        "        s_out = self.dropout(self.s_unit(x))\n",
        "        x = self.dropout(self.t_unit(s_out) + self.residual(x[:, :, self.pad : x.shape[2] - self.pad, :]))\n",
        "        return self.relu(x)\n",
        "\n",
        "\n",
        "class UNIK(nn.Module):\n",
        "    def __init__(self, num_class=49, num_joints=20, num_heads=3, in_channels=3):\n",
        "        super(UNIK, self).__init__()\n",
        "\n",
        "        self.data_bn = nn.BatchNorm1d(in_channels * num_joints)\n",
        "\n",
        "        self.l1 = ST_block(in_channels, 64, num_joints, residual=False)\n",
        "        self.l2 = ST_block(64, 128, num_joints, num_heads, stride=2)\n",
        "        self.l3 = ST_block(128, 256, num_joints, num_heads, stride=2)\n",
        "        self.l4 = ST_block(256, 128, num_joints, num_heads)\n",
        "        self.fc = nn.Linear(128, num_class)\n",
        "        nn.init.normal_(self.fc.weight, 0, math.sqrt(2. / num_class))\n",
        "        bn_init(self.data_bn, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        N, C, V, T = x.size()\n",
        "        x = x.permute(0,2,1,3).contiguous().view(N,V*C,T)\n",
        "        x = self.data_bn(x)\n",
        "        x = x.view(N,V,C,T).permute(0,2,3,1).contiguous().view(N,C,T,V)\n",
        "\n",
        "        x = self.l1(x)\n",
        "        x = self.l2(x)\n",
        "        x = self.l3(x)\n",
        "        x = self.l4(x)\n",
        "        x=x.view(N,x.size(1),-1)\n",
        "        x = x.mean(-1)\n",
        "\n",
        "        return self.fc(x)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BaphY4zubAp3"
      },
      "source": [
        "class GCNDataset(Dataset):\n",
        "    def __init__(self,filename,hasLabel=True,aug=False):\n",
        "        self.df = pd.read_csv(filename,header=None)\n",
        "        self.length = len(self.df)\n",
        "        if hasLabel:\n",
        "            #self.df['freq']=1./self.df.groupby(961)[961].transform('count')\n",
        "            #self.df = self.df.sample(frac=1,weights=self.df.freq).reset_index(drop=True)\n",
        "            self.df = self.df.sample(frac=1,random_state=2021).reset_index(drop=True)\n",
        "            self.X = torch.tensor(self.df.iloc[:,1:-1].values.astype('float32'))\n",
        "            # N, C, V, T\n",
        "            self.X = self.X.reshape((self.length ,16, 20, 3)).permute(0,3,2,1).contiguous()\n",
        "            if aug:\n",
        "                X1=self.X\n",
        "                X2=self.X\n",
        "                X3=self.X\n",
        "                X1[:,0,:,:] *= -1\n",
        "                X2[:,1,:,:] *= -1\n",
        "                X3[:,2,:,:] *= -1\n",
        "                self.X = torch.cat((X1,X2,X3,self.X),0)\n",
        "            self.labels = self.df.iloc[:,-1].values.astype('int32')-1\n",
        "            self.Y = torch.tensor(self.labels,dtype=torch.long)\n",
        "        else:\n",
        "            self.X = torch.tensor(self.df.iloc[:,1:].values.astype('float32'))\n",
        "            self.X = self.X.reshape((self.length ,16, 20, 3)).permute(0,3,2,1).contiguous()\n",
        "            self.Y=torch.tensor(np.zeros(self.length),dtype=torch.long)\n",
        "\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    \n",
        "    def __getitem__(self,index):\n",
        "        x = self.X[index]\n",
        "        y = self.Y[index]\n",
        "        return x, y "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9yV7b5ibLaE"
      },
      "source": [
        "root_path = \"drive/MyDrive/SML/\"\n",
        "bs=64\n",
        "train_set = GCNDataset(filename=root_path+\"training_set.csv\",aug=False)\n",
        "val_set = GCNDataset(filename=root_path+\"val_set.csv\")\n",
        "test_set = GCNDataset(filename=root_path+\"test.csv\",hasLabel=False)\n",
        "train_loader = DataLoader(train_set,batch_size=bs,shuffle=True)\n",
        "val_loader = DataLoader(val_set,batch_size=bs)\n",
        "test_loader = DataLoader(test_set,batch_size=bs)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CovbgOkq28HV",
        "outputId": "d0645760-3dcc-4dcc-86d1-07c6f59edc53"
      },
      "source": [
        "#!pip install adabound"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting adabound\n",
            "  Downloading adabound-0.0.5-py3-none-any.whl (5.1 kB)\n",
            "Requirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from adabound) (1.9.0+cu102)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.0->adabound) (3.7.4.3)\n",
            "Installing collected packages: adabound\n",
            "Successfully installed adabound-0.0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2fXqnK7c_Yc",
        "outputId": "a0d9e5fe-c511-4f39-cf92-821b9a0b425c"
      },
      "source": [
        "#import adabound\n",
        "net = UNIK()\n",
        "#net.load_state_dict(torch.load(root_path+'UNIK_best.pkl'))\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "gpu = 0 #gpu ID\n",
        "net.cuda(gpu)\n",
        "#opti = optim.SGD(net.parameters(),lr=0.01, weight_decay=0.0005)\n",
        "opti = optim.Adam(net.parameters(),lr=0.01,weight_decay=0.0005)\n",
        "#opti.load_state_dict(torch.load(root_path+'UNIK_best_optim.pkl'))\n",
        "#opti=adabound.AdaBound(net.parameters(),lr=0.1,weight_decay=0.0001, final_lr=0.1)\n",
        "print()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nkJPZ0n9ds9U",
        "outputId": "fbdd96fa-616f-4497-b0a4-838548d08d11"
      },
      "source": [
        "def adjust_learning_rate(lr,wd=0.0005):\n",
        "    for param_group in opti.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "        param_group['weight_decay'] = wd\n",
        "    return lr\n",
        "\n",
        "def accuracy(logit,target):\n",
        "    a=(torch.argmax(logit,dim=1)==target).sum()\n",
        "    return a\n",
        "\n",
        "def evaluate(model, criterion, dataloader, gpu):\n",
        "    model.eval()\n",
        "    acc = 0\n",
        "    count = 0\n",
        "    with torch.no_grad():\n",
        "        for i,(x,y) in enumerate(dataloader):\n",
        "            x,y = x.cuda(gpu), y.cuda(gpu)\n",
        "            logits = model(x)\n",
        "            acc+= accuracy(logits, y)\n",
        "            count += bs\n",
        "\n",
        "    return acc / count\n",
        "\n",
        "def train():\n",
        "    best_acc= 0.57  \n",
        "    best_epoch = 0\n",
        "    for epoch in range(200):\n",
        "        total = 0\n",
        "        correct = 0\n",
        "\n",
        "        if epoch==0:\n",
        "            adjust_learning_rate(0.01)\n",
        "        elif epoch==80:\n",
        "            adjust_learning_rate(0.001)\n",
        "        elif epoch==160:\n",
        "            adjust_learning_rate(0.0001)\n",
        "        elif epoch==180:\n",
        "            adjust_learning_rate(0.00001)\n",
        "\n",
        "        for i, (x,y) in enumerate(train_loader):\n",
        "            net.train()\n",
        "            opti.zero_grad()\n",
        "            x,y=x.cuda(gpu),y.cuda(gpu)\n",
        "            logit = net(x)\n",
        "            loss = criterion(logit,y)\n",
        "            loss.backward()\n",
        "            opti.step()\n",
        "            correct+=accuracy(logit,y)\n",
        "            total+=bs\n",
        "        dev_acc = evaluate(net, criterion, val_loader, gpu)\n",
        "        \n",
        "        if dev_acc>best_acc:\n",
        "            best_acc=dev_acc\n",
        "            torch.save(net.state_dict(), root_path+'UNIK_best.pkl')\n",
        "            torch.save(opti.state_dict(), root_path+\"UNIK_best_optim.pkl\")\n",
        "        print(\"epoch\",epoch,\"train acc:\",round(float(correct/total),5),\"dev_acc:\",round(float(dev_acc),5))\n",
        "train()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0 train acc: 0.17823 dev_acc: 0.18281\n",
            "epoch 1 train acc: 0.26496 dev_acc: 0.26458\n",
            "epoch 2 train acc: 0.31051 dev_acc: 0.30729\n",
            "epoch 3 train acc: 0.34031 dev_acc: 0.34844\n",
            "epoch 4 train acc: 0.35262 dev_acc: 0.34948\n",
            "epoch 5 train acc: 0.36745 dev_acc: 0.38021\n",
            "epoch 6 train acc: 0.37301 dev_acc: 0.35938\n",
            "epoch 7 train acc: 0.38678 dev_acc: 0.36719\n",
            "epoch 8 train acc: 0.40201 dev_acc: 0.39844\n",
            "epoch 9 train acc: 0.39857 dev_acc: 0.3875\n",
            "epoch 10 train acc: 0.41684 dev_acc: 0.41927\n",
            "epoch 11 train acc: 0.42624 dev_acc: 0.4151\n",
            "epoch 12 train acc: 0.42545 dev_acc: 0.40729\n",
            "epoch 13 train acc: 0.43379 dev_acc: 0.36146\n",
            "epoch 14 train acc: 0.43803 dev_acc: 0.42708\n",
            "epoch 15 train acc: 0.4371 dev_acc: 0.43958\n",
            "epoch 16 train acc: 0.442 dev_acc: 0.44635\n",
            "epoch 17 train acc: 0.44743 dev_acc: 0.44323\n",
            "epoch 18 train acc: 0.44756 dev_acc: 0.43385\n",
            "epoch 19 train acc: 0.4526 dev_acc: 0.44896\n",
            "epoch 20 train acc: 0.46319 dev_acc: 0.44115\n",
            "epoch 21 train acc: 0.45233 dev_acc: 0.44531\n",
            "epoch 22 train acc: 0.46001 dev_acc: 0.47135\n",
            "epoch 23 train acc: 0.46359 dev_acc: 0.46615\n",
            "epoch 24 train acc: 0.45842 dev_acc: 0.45313\n",
            "epoch 25 train acc: 0.46517 dev_acc: 0.44844\n",
            "epoch 26 train acc: 0.46306 dev_acc: 0.41771\n",
            "epoch 27 train acc: 0.47166 dev_acc: 0.44219\n",
            "epoch 28 train acc: 0.46954 dev_acc: 0.42292\n",
            "epoch 29 train acc: 0.47378 dev_acc: 0.46354\n",
            "epoch 30 train acc: 0.47842 dev_acc: 0.44323\n",
            "epoch 31 train acc: 0.47233 dev_acc: 0.43698\n",
            "epoch 32 train acc: 0.46875 dev_acc: 0.45052\n",
            "epoch 33 train acc: 0.48173 dev_acc: 0.4599\n",
            "epoch 34 train acc: 0.47842 dev_acc: 0.45833\n",
            "epoch 35 train acc: 0.47842 dev_acc: 0.46875\n",
            "epoch 36 train acc: 0.48053 dev_acc: 0.475\n",
            "epoch 37 train acc: 0.48173 dev_acc: 0.48906\n",
            "epoch 38 train acc: 0.48941 dev_acc: 0.45625\n",
            "epoch 39 train acc: 0.48782 dev_acc: 0.44635\n",
            "epoch 40 train acc: 0.48649 dev_acc: 0.50104\n",
            "epoch 41 train acc: 0.49073 dev_acc: 0.43958\n",
            "epoch 42 train acc: 0.48265 dev_acc: 0.46667\n",
            "epoch 43 train acc: 0.48954 dev_acc: 0.45208\n",
            "epoch 44 train acc: 0.49033 dev_acc: 0.46198\n",
            "epoch 45 train acc: 0.48927 dev_acc: 0.46771\n",
            "epoch 46 train acc: 0.48901 dev_acc: 0.45625\n",
            "epoch 47 train acc: 0.49139 dev_acc: 0.42917\n",
            "epoch 48 train acc: 0.4906 dev_acc: 0.47656\n",
            "epoch 49 train acc: 0.49841 dev_acc: 0.47708\n",
            "epoch 50 train acc: 0.49258 dev_acc: 0.47604\n",
            "epoch 51 train acc: 0.49484 dev_acc: 0.45833\n",
            "epoch 52 train acc: 0.50013 dev_acc: 0.4849\n",
            "epoch 53 train acc: 0.49351 dev_acc: 0.47917\n",
            "epoch 54 train acc: 0.50093 dev_acc: 0.45104\n",
            "epoch 55 train acc: 0.49642 dev_acc: 0.48698\n",
            "epoch 56 train acc: 0.50424 dev_acc: 0.47083\n",
            "epoch 57 train acc: 0.49245 dev_acc: 0.46979\n",
            "epoch 58 train acc: 0.50252 dev_acc: 0.45677\n",
            "epoch 59 train acc: 0.50079 dev_acc: 0.49167\n",
            "epoch 60 train acc: 0.49682 dev_acc: 0.49792\n",
            "epoch 61 train acc: 0.5004 dev_acc: 0.50052\n",
            "epoch 62 train acc: 0.49311 dev_acc: 0.48958\n",
            "epoch 63 train acc: 0.49921 dev_acc: 0.47344\n",
            "epoch 64 train acc: 0.50331 dev_acc: 0.47083\n",
            "epoch 65 train acc: 0.49576 dev_acc: 0.49063\n",
            "epoch 66 train acc: 0.4996 dev_acc: 0.42083\n",
            "epoch 67 train acc: 0.49775 dev_acc: 0.49844\n",
            "epoch 68 train acc: 0.49669 dev_acc: 0.48073\n",
            "epoch 69 train acc: 0.49775 dev_acc: 0.49844\n",
            "epoch 70 train acc: 0.50093 dev_acc: 0.49531\n",
            "epoch 71 train acc: 0.51165 dev_acc: 0.49479\n",
            "epoch 72 train acc: 0.50953 dev_acc: 0.48646\n",
            "epoch 73 train acc: 0.50662 dev_acc: 0.46823\n",
            "epoch 74 train acc: 0.50649 dev_acc: 0.48281\n",
            "epoch 75 train acc: 0.50331 dev_acc: 0.48333\n",
            "epoch 76 train acc: 0.50291 dev_acc: 0.43958\n",
            "epoch 77 train acc: 0.51377 dev_acc: 0.46823\n",
            "epoch 78 train acc: 0.50649 dev_acc: 0.4974\n",
            "epoch 79 train acc: 0.50609 dev_acc: 0.50469\n",
            "epoch 80 train acc: 0.54568 dev_acc: 0.52344\n",
            "epoch 81 train acc: 0.57296 dev_acc: 0.53594\n",
            "epoch 82 train acc: 0.56965 dev_acc: 0.53854\n",
            "epoch 83 train acc: 0.57296 dev_acc: 0.53073\n",
            "epoch 84 train acc: 0.58276 dev_acc: 0.53438\n",
            "epoch 85 train acc: 0.57905 dev_acc: 0.53177\n",
            "epoch 86 train acc: 0.57998 dev_acc: 0.53854\n",
            "epoch 87 train acc: 0.58832 dev_acc: 0.54479\n",
            "epoch 88 train acc: 0.58991 dev_acc: 0.54115\n",
            "epoch 89 train acc: 0.58832 dev_acc: 0.5474\n",
            "epoch 90 train acc: 0.58912 dev_acc: 0.53385\n",
            "epoch 91 train acc: 0.59481 dev_acc: 0.5375\n",
            "epoch 92 train acc: 0.59746 dev_acc: 0.54323\n",
            "epoch 93 train acc: 0.59216 dev_acc: 0.54427\n",
            "epoch 94 train acc: 0.59878 dev_acc: 0.54635\n",
            "epoch 95 train acc: 0.59799 dev_acc: 0.54479\n",
            "epoch 96 train acc: 0.596 dev_acc: 0.55208\n",
            "epoch 97 train acc: 0.59971 dev_acc: 0.55365\n",
            "epoch 98 train acc: 0.59362 dev_acc: 0.54531\n",
            "epoch 99 train acc: 0.60434 dev_acc: 0.5526\n",
            "epoch 100 train acc: 0.60169 dev_acc: 0.54375\n",
            "epoch 101 train acc: 0.59891 dev_acc: 0.54375\n",
            "epoch 102 train acc: 0.60275 dev_acc: 0.54479\n",
            "epoch 103 train acc: 0.60355 dev_acc: 0.54427\n",
            "epoch 104 train acc: 0.6054 dev_acc: 0.54427\n",
            "epoch 105 train acc: 0.60885 dev_acc: 0.54271\n",
            "epoch 106 train acc: 0.6005 dev_acc: 0.53854\n",
            "epoch 107 train acc: 0.6099 dev_acc: 0.54323\n",
            "epoch 108 train acc: 0.61229 dev_acc: 0.54375\n",
            "epoch 109 train acc: 0.6058 dev_acc: 0.55052\n",
            "epoch 110 train acc: 0.60805 dev_acc: 0.55208\n",
            "epoch 111 train acc: 0.61533 dev_acc: 0.55313\n",
            "epoch 112 train acc: 0.60951 dev_acc: 0.55156\n",
            "epoch 113 train acc: 0.61308 dev_acc: 0.54427\n",
            "epoch 114 train acc: 0.6103 dev_acc: 0.55313\n",
            "epoch 115 train acc: 0.60951 dev_acc: 0.54063\n",
            "epoch 116 train acc: 0.60924 dev_acc: 0.55313\n",
            "epoch 117 train acc: 0.61547 dev_acc: 0.54688\n",
            "epoch 118 train acc: 0.61454 dev_acc: 0.55365\n",
            "epoch 119 train acc: 0.61586 dev_acc: 0.54635\n",
            "epoch 120 train acc: 0.61639 dev_acc: 0.54531\n",
            "epoch 121 train acc: 0.6152 dev_acc: 0.55469\n",
            "epoch 122 train acc: 0.61878 dev_acc: 0.54792\n",
            "epoch 123 train acc: 0.62553 dev_acc: 0.54167\n",
            "epoch 124 train acc: 0.61891 dev_acc: 0.55052\n",
            "epoch 125 train acc: 0.62328 dev_acc: 0.54844\n",
            "epoch 126 train acc: 0.6205 dev_acc: 0.55156\n",
            "epoch 127 train acc: 0.61957 dev_acc: 0.5401\n",
            "epoch 128 train acc: 0.61547 dev_acc: 0.54271\n",
            "epoch 129 train acc: 0.62076 dev_acc: 0.55052\n",
            "epoch 130 train acc: 0.62156 dev_acc: 0.56042\n",
            "epoch 131 train acc: 0.62434 dev_acc: 0.54948\n",
            "epoch 132 train acc: 0.62169 dev_acc: 0.56302\n",
            "epoch 133 train acc: 0.62844 dev_acc: 0.54115\n",
            "epoch 134 train acc: 0.62394 dev_acc: 0.57292\n",
            "epoch 135 train acc: 0.6291 dev_acc: 0.54323\n",
            "epoch 136 train acc: 0.6197 dev_acc: 0.54635\n",
            "epoch 137 train acc: 0.62447 dev_acc: 0.55521\n",
            "epoch 138 train acc: 0.62182 dev_acc: 0.56458\n",
            "epoch 139 train acc: 0.62368 dev_acc: 0.55313\n",
            "epoch 140 train acc: 0.62381 dev_acc: 0.55313\n",
            "epoch 141 train acc: 0.62818 dev_acc: 0.5599\n",
            "epoch 142 train acc: 0.63599 dev_acc: 0.55313\n",
            "epoch 143 train acc: 0.63175 dev_acc: 0.55104\n",
            "epoch 144 train acc: 0.63506 dev_acc: 0.55\n",
            "epoch 145 train acc: 0.62434 dev_acc: 0.54635\n",
            "epoch 146 train acc: 0.63149 dev_acc: 0.54323\n",
            "epoch 147 train acc: 0.62844 dev_acc: 0.5599\n",
            "epoch 148 train acc: 0.63626 dev_acc: 0.55833\n",
            "epoch 149 train acc: 0.63599 dev_acc: 0.5599\n",
            "epoch 150 train acc: 0.62685 dev_acc: 0.5474\n",
            "epoch 151 train acc: 0.63798 dev_acc: 0.54479\n",
            "epoch 152 train acc: 0.63467 dev_acc: 0.56094\n",
            "epoch 153 train acc: 0.6291 dev_acc: 0.55833\n",
            "epoch 154 train acc: 0.62977 dev_acc: 0.5526\n",
            "epoch 155 train acc: 0.63281 dev_acc: 0.55573\n",
            "epoch 156 train acc: 0.62884 dev_acc: 0.55521\n",
            "epoch 157 train acc: 0.63109 dev_acc: 0.55365\n",
            "epoch 158 train acc: 0.63149 dev_acc: 0.53906\n",
            "epoch 159 train acc: 0.63533 dev_acc: 0.56563\n",
            "epoch 160 train acc: 0.65003 dev_acc: 0.56875\n",
            "epoch 161 train acc: 0.65215 dev_acc: 0.5651\n",
            "epoch 162 train acc: 0.64817 dev_acc: 0.56302\n",
            "epoch 163 train acc: 0.6585 dev_acc: 0.56771\n",
            "epoch 164 train acc: 0.6585 dev_acc: 0.5724\n",
            "epoch 165 train acc: 0.65572 dev_acc: 0.5724\n",
            "epoch 166 train acc: 0.65678 dev_acc: 0.57448\n",
            "epoch 167 train acc: 0.65585 dev_acc: 0.56302\n",
            "epoch 168 train acc: 0.65122 dev_acc: 0.56875\n",
            "epoch 169 train acc: 0.65863 dev_acc: 0.57917\n",
            "epoch 170 train acc: 0.66327 dev_acc: 0.56667\n",
            "epoch 171 train acc: 0.65824 dev_acc: 0.57604\n",
            "epoch 172 train acc: 0.65334 dev_acc: 0.57188\n",
            "epoch 173 train acc: 0.65281 dev_acc: 0.5625\n",
            "epoch 174 train acc: 0.66499 dev_acc: 0.56823\n",
            "epoch 175 train acc: 0.66446 dev_acc: 0.56875\n",
            "epoch 176 train acc: 0.66314 dev_acc: 0.56719\n",
            "epoch 177 train acc: 0.66314 dev_acc: 0.56875\n",
            "epoch 178 train acc: 0.6581 dev_acc: 0.575\n",
            "epoch 179 train acc: 0.66923 dev_acc: 0.57813\n",
            "epoch 180 train acc: 0.66419 dev_acc: 0.57083\n",
            "epoch 181 train acc: 0.67188 dev_acc: 0.5724\n",
            "epoch 182 train acc: 0.6634 dev_acc: 0.56927\n",
            "epoch 183 train acc: 0.67386 dev_acc: 0.56042\n",
            "epoch 184 train acc: 0.66247 dev_acc: 0.57292\n",
            "epoch 185 train acc: 0.66751 dev_acc: 0.575\n",
            "epoch 186 train acc: 0.66817 dev_acc: 0.56667\n",
            "epoch 187 train acc: 0.66088 dev_acc: 0.56667\n",
            "epoch 188 train acc: 0.66128 dev_acc: 0.5724\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-97a97964ff43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopti\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"UNIK_best_optim.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"epoch\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"train acc:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrect\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"dev_acc:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-97a97964ff43>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0mopti\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mlogit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mzero_grad\u001b[0;34m(self, set_to_none)\u001b[0m\n\u001b[1;32m    214\u001b[0m                             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    983\u001b[0m                           \u001b[0;34m\"non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m                           \"github.com/pytorch/pytorch/pull/30531 for more information.\", stacklevel=2)\n\u001b[0;32m--> 985\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EWa0t0MbdW9"
      },
      "source": [
        "torch.save(net.state_dict(), root_path+'UNIK_best.pkl')\n",
        "torch.save(opti.state_dict(), root_path+\"UNIK_best_optim.pkl\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35vWxYnGxncv",
        "outputId": "af12eaa1-f6a2-47f1-ce79-302dd395e275"
      },
      "source": [
        "def predict(net,dataloader):\n",
        "    net.eval()\n",
        "    predictions = torch.tensor([]).cuda(gpu)\n",
        "    logits = torch.tensor([]).cuda(gpu)\n",
        "    with torch.no_grad():\n",
        "        for i, (x,y) in enumerate(dataloader):\n",
        "            x,y=x.cuda(gpu),y.cuda(gpu)\n",
        "            logit = net(x)\n",
        "            logits = torch.cat((logits,logit))\n",
        "            pred = torch.argmax(logit,dim=1)+1\n",
        "            predictions=torch.cat((predictions,pred))\n",
        "    return predictions.cpu().numpy(), logits.cpu().numpy()\n",
        "pred,logits = predict(net,test_loader)\n",
        "print(pred,logits.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 1. 11.  9. ... 14. 10. 30.] (2959, 49)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UNamZ4Z2qOqN",
        "outputId": "80f05bf8-5d32-4eef-bafa-c16354f0c9f1"
      },
      "source": [
        "\"\"\"\n",
        "_,logits = predict(net,train_loader)\n",
        "train_logits = pd.DataFrame(logits)\n",
        "train_logits.to_csv(\"logits_train_unik1.csv\",index=None,header=None)\n",
        "_,logits = predict(net,test_loader)\n",
        "test_logits = pd.DataFrame(logits)\n",
        "test_logits.to_csv(\"logits_test_unik1.csv\",index=None,header=None)\n",
        "print(train_logits.shape,test_logits.shape)\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(9388, 49)\n",
            "(9388, 49) (2959, 49)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tiSrPYEvx2YX"
      },
      "source": [
        "output = pd.read_csv(root_path+\"sample.csv\")\n",
        "output['Category']=pred.astype('int')\n",
        "output.head()\n",
        "output.to_csv(\"predictions.csv\",index=None)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}