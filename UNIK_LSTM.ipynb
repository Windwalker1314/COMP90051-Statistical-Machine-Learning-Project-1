{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "UNIK_LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4F8Ry34BU8N8"
      },
      "source": [
        "import math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.utils import shuffle"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OaqD2_MHVIIL",
        "outputId": "3addf67e-5c62-4d73-9a4a-e5401e306af1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "gpu = 0 #gpu ID"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXGbR7L7VI2l"
      },
      "source": [
        "def conv_branch_init(conv, branches):\n",
        "    weight = conv.weight\n",
        "    n = weight.size(0)\n",
        "    k1 = weight.size(1)\n",
        "    k2 = weight.size(2)\n",
        "    nn.init.normal_(weight, 0, math.sqrt(2. / (n * k1 * k2 * branches)))\n",
        "    nn.init.constant_(conv.bias, 0)\n",
        "\n",
        "\n",
        "def conv_init(conv):\n",
        "    nn.init.kaiming_normal_(conv.weight, mode='fan_out')\n",
        "    nn.init.constant_(conv.bias, 0)\n",
        "\n",
        "\n",
        "def bn_init(bn, scale):\n",
        "    nn.init.constant_(bn.weight, scale)\n",
        "    nn.init.constant_(bn.bias, 0)\n",
        "\n",
        "\n",
        "        \n",
        "\n",
        "# Temporal unit\n",
        "class T_LSU(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, T=16,V=20,return_seq=True):\n",
        "        super(T_LSU, self).__init__()\n",
        "        self.V=V\n",
        "        self.out_channels = out_channels\n",
        "        self.in_channels = in_channels\n",
        "        self.return_seq = return_seq\n",
        "        self.relu = nn.ReLU()\n",
        "        self.lr = nn.Linear(out_channels*V, out_channels)\n",
        "        self.lstm_layers=nn.ModuleList([nn.LSTM(in_channels,out_channels,num_layers=1,batch_first=True) for i in range(V)])\n",
        "    def forward(self, x):\n",
        "        # N, C, T, V\n",
        "        N,C,T,V = x.size()\n",
        "        x = x.permute(0,3,2,1).contiguous().view(N,V,T,C)\n",
        "        output = torch.tensor([])\n",
        "        if gpu is not None:\n",
        "            output = output.cuda(gpu)\n",
        "        assert C==self.in_channels\n",
        "        for i in range(V):\n",
        "            output_i,_ = self.lstm_layers[i](x[:,i,:,:])\n",
        "            output_i = output_i.unsqueeze(1)\n",
        "            if gpu is not None:\n",
        "                output_i.cuda(gpu)\n",
        "            output = torch.cat([output,output_i],dim=1)\n",
        "        N,V,T,C = output.size()\n",
        "        if self.return_seq:\n",
        "            x = output.permute(0,3,2,1).contiguous().view(N,C,T,V)\n",
        "            # N,C,T,V\n",
        "            \n",
        "            return x\n",
        "\n",
        "        # N, V, C\n",
        "        x = output[:,:,-1,:]\n",
        "        x = x.contiguous().view(N,V*C)\n",
        "        x = self.lr(x)\n",
        "        return self.relu(x)\n",
        "\n",
        "# Spatial unit\n",
        "class S_LSU(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, num_joints, num_heads=8, coff_embedding=4, bias=True):\n",
        "        super(S_LSU, self).__init__()\n",
        "        inter_channels = out_channels // coff_embedding\n",
        "        self.inter_c = inter_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.num_heads = num_heads\n",
        "        self.DepM = nn.Parameter(torch.Tensor(num_heads, num_joints, num_joints))\n",
        "        if bias:\n",
        "            self.bias = nn.Parameter(torch.Tensor(num_joints))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "        self.reset_parameters()\n",
        "\n",
        "        # Attention\n",
        "        self.conv_a = nn.ModuleList()\n",
        "        self.conv_b = nn.ModuleList()\n",
        "        self.conv_d = nn.ModuleList()\n",
        "        for i in range(self.num_heads):\n",
        "            self.conv_a.append(nn.Conv2d(in_channels, inter_channels, 1))\n",
        "            self.conv_b.append(nn.Conv2d(in_channels, inter_channels, 1))\n",
        "            self.conv_d.append(nn.Conv2d(in_channels, out_channels, 1))\n",
        "\n",
        "        if in_channels != out_channels:\n",
        "            self.down = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, 1),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "        else:\n",
        "            self.down = lambda x: x\n",
        "\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.soft = nn.Softmax(-2)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                conv_init(m)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                bn_init(m, 1)\n",
        "        bn_init(self.bn, 1e-6)\n",
        "        for i in range(self.num_heads):\n",
        "            conv_branch_init(self.conv_d[i], self.num_heads)\n",
        "\n",
        "\n",
        "    def reset_parameters(self) -> None:\n",
        "        nn.init.kaiming_uniform_(self.DepM, a=math.sqrt(5))\n",
        "        if self.bias is not None:\n",
        "            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.DepM)\n",
        "            bound = 1 / math.sqrt(fan_in)\n",
        "            nn.init.uniform_(self.bias, -bound, bound)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        N, C, T, V = x.size()\n",
        "        W = self.DepM\n",
        "        B = self.bias\n",
        "        y = None\n",
        "        for i in range(self.num_heads):\n",
        "\n",
        "            A1 = self.conv_a[i](x).permute(0, 3, 1, 2).contiguous().view(N, V, self.inter_c * T)\n",
        "            A2 = self.conv_b[i](x).view(N, self.inter_c * T, V)\n",
        "            A1 = self.soft(torch.matmul(A1, A2) / A1.size(-1))  # N tV tV\n",
        "\n",
        "            A1 = W[i] + A1\n",
        "            A2 = x.view(N, C * T, V)\n",
        "            z = self.conv_d[i]((torch.matmul(A2, A1)).view(N, C, T, V))\n",
        "            y = z + y if y is not None else z\n",
        "\n",
        "        y = self.bn(y)\n",
        "        y += self.down(x)\n",
        "        return self.relu(y).view(N, -1, T, V)\n",
        "\n",
        "\n",
        "class ST_block(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, num_joints=25, num_heads=3,residual=True,return_seq=True):\n",
        "        super(ST_block, self).__init__()\n",
        "        self.s_unit = S_LSU(in_channels, out_channels, num_joints, num_heads)\n",
        "        self.t_unit = T_LSU(out_channels, out_channels,return_seq=return_seq)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "    \n",
        "        if residual:\n",
        "            self.residual = lambda x: x\n",
        "        else:\n",
        "            self.residual = lambda x: 0\n",
        "\n",
        "    def forward(self, x):\n",
        "        s_out = self.dropout(self.s_unit(x))\n",
        "        x = self.dropout(self.t_unit(s_out) + self.residual(s_out))\n",
        "        return self.relu(x)\n",
        "\n",
        "\n",
        "class UNIK(nn.Module):\n",
        "    def __init__(self, num_class=49, num_joints=20, num_heads=3, in_channels=3):\n",
        "        super(UNIK, self).__init__()\n",
        "\n",
        "        self.data_bn = nn.BatchNorm1d(in_channels * num_joints)\n",
        "\n",
        "        self.l1 = ST_block(in_channels, 64, num_joints, residual=False)\n",
        "        self.l2 = ST_block(64, 128, num_joints, num_heads)\n",
        "        self.l3 = ST_block(128, 256, num_joints, num_heads)\n",
        "        self.l4 = ST_block(256, 128, num_joints, num_heads, return_seq = False,residual=False)\n",
        "        self.fc = nn.Linear(128, num_class)\n",
        "        nn.init.normal_(self.fc.weight, 0, math.sqrt(2. / num_class))\n",
        "        bn_init(self.data_bn, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        N, C, V, T = x.size()\n",
        "        x = x.permute(0,2,1,3).contiguous().view(N,V*C,T)\n",
        "        x = self.data_bn(x)\n",
        "        x = x.view(N,V,C,T).permute(0,2,3,1).contiguous().view(N,C,T,V)\n",
        "\n",
        "        x = self.l1(x)\n",
        "        x = self.l2(x)\n",
        "        x = self.l3(x)\n",
        "        x = self.l4(x)\n",
        "        x=x.view(N,-1)\n",
        "\n",
        "        return self.fc(x)"
      ],
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwrU8Oc8VT5_"
      },
      "source": [
        "class GCNDataset(Dataset):\n",
        "    def __init__(self,filename,hasLabel=True,aug=False):\n",
        "        self.df = pd.read_csv(filename,header=None)\n",
        "        self.length = len(self.df)\n",
        "        if hasLabel:\n",
        "            #self.df['freq']=1./self.df.groupby(961)[961].transform('count')\n",
        "            #self.df = self.df.sample(frac=1,weights=self.df.freq).reset_index(drop=True)\n",
        "            self.df = self.df.sample(frac=1).reset_index(drop=True)\n",
        "            self.X = torch.tensor(self.df.iloc[:,1:-1].values.astype('float32'))\n",
        "            # N, C, V, T\n",
        "            self.X = self.X.reshape((self.length ,16, 20, 3)).permute(0,3,2,1).contiguous()\n",
        "            if aug:\n",
        "                X1=self.X\n",
        "                X2=self.X\n",
        "                X3=self.X\n",
        "                X1[:,0,:,:] *= -1\n",
        "                X2[:,1,:,:] *= -1\n",
        "                X3[:,2,:,:] *= -1\n",
        "                self.X = torch.cat((X1,X2,X3,self.X),0)\n",
        "            self.labels = self.df.iloc[:,-1].values.astype('int32')-1\n",
        "            self.Y = torch.tensor(self.labels,dtype=torch.long)\n",
        "        else:\n",
        "            self.X = torch.tensor(self.df.iloc[:,1:].values.astype('float32'))\n",
        "            self.X = self.X.reshape((self.length ,16, 20, 3)).permute(0,3,2,1).contiguous()\n",
        "            self.Y=torch.tensor(np.zeros(self.length),dtype=torch.long)\n",
        "\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    \n",
        "    def __getitem__(self,index):\n",
        "        x = self.X[index]\n",
        "        y = self.Y[index]\n",
        "        return x, y "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogfngiKqVXA2"
      },
      "source": [
        "root_path = \"drive/MyDrive/SML/\"\n",
        "bs=64\n",
        "train_set = GCNDataset(filename=root_path+\"training_set.csv\",aug=False)\n",
        "val_set = GCNDataset(filename=root_path+\"val_set.csv\")\n",
        "test_set = GCNDataset(filename=root_path+\"test.csv\",hasLabel=False)\n",
        "train_loader = DataLoader(train_set,batch_size=bs,num_workers=2,shuffle=True)\n",
        "val_loader = DataLoader(val_set,batch_size=bs)\n",
        "test_loader = DataLoader(test_set,batch_size=bs)"
      ],
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7PL_nOVVXkM",
        "outputId": "5523f89f-fcc6-4ffe-cb88-08a96d6567e1"
      },
      "source": [
        "net = UNIK()\n",
        "net.load_state_dict(torch.load(root_path+'UNIK_r_best.pkl'))\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "net.cuda(gpu)\n",
        "#opti = optim.SGD(net.parameters(),lr=0.01, weight_decay=0.0005)\n",
        "opti = optim.Adam(net.parameters(),lr=0.01,weight_decay=0.0005)\n",
        "opti.load_state_dict(torch.load(root_path+'UNIK_r_optim.pkl'))\n",
        "print()"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        },
        "id": "cMxnHZh5Vge4",
        "outputId": "ccf70a50-0d5b-4b20-b709-96a29f673aae"
      },
      "source": [
        "def adjust_learning_rate(lr,wd):\n",
        "    for param_group in opti.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "    return lr\n",
        "adjust_learning_rate(5e-5,0.0002)\n",
        "\n",
        "def accuracy(logit,target):\n",
        "    a=(torch.argmax(logit,dim=1)==target).sum()\n",
        "    return a\n",
        "\n",
        "def evaluate(model, criterion, dataloader, gpu):\n",
        "    model.eval()\n",
        "    acc = 0\n",
        "    count = 0\n",
        "    with torch.no_grad():\n",
        "        for i,(x,y) in enumerate(dataloader):\n",
        "            x,y = x.cuda(gpu), y.cuda(gpu)\n",
        "            logits = model(x)\n",
        "            acc+= accuracy(logits, y)\n",
        "            count += bs\n",
        "\n",
        "    return acc / count\n",
        "\n",
        "def train():\n",
        "    best_acc=0.494\n",
        "    best_epoch = 0\n",
        "    for epoch in range(200):\n",
        "        total = 0\n",
        "        correct = 0\n",
        "        \"\"\"\n",
        "        if epoch==0:\n",
        "            adjust_learning_rate(0.01)\n",
        "        elif epoch==20:\n",
        "            adjust_learning_rate(0.001)\n",
        "        elif epoch==40:\n",
        "            adjust_learning_rate(0.0001)\n",
        "        elif epoch==60:\n",
        "            adjust_learning_rate(0.00001)\n",
        "        \"\"\"\n",
        "        for i, (x,y) in enumerate(train_loader):\n",
        "            net.train()\n",
        "            opti.zero_grad()\n",
        "            x,y=x.cuda(gpu),y.cuda(gpu)\n",
        "            logit = net(x)\n",
        "            loss = criterion(logit,y)\n",
        "            loss.backward()\n",
        "            opti.step()\n",
        "            correct+=accuracy(logit,y)\n",
        "            total+=bs\n",
        "        dev_acc = evaluate(net, criterion, val_loader, gpu)\n",
        "        \n",
        "        if dev_acc>best_acc:\n",
        "            best_acc=dev_acc\n",
        "            torch.save(net.state_dict(), root_path+'UNIK_r_best.pkl')\n",
        "            torch.save(opti.state_dict(), root_path+\"UNIK_r_optim.pkl\")\n",
        "        print(\"epoch\",epoch,\"train acc:\",round(float(correct/total),5),\"dev_acc:\",round(float(dev_acc),5))\n",
        "train()"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0 train acc: 0.66155 dev_acc: 0.48646\n",
            "epoch 1 train acc: 0.66274 dev_acc: 0.48646\n",
            "epoch 2 train acc: 0.66817 dev_acc: 0.4875\n",
            "epoch 3 train acc: 0.66565 dev_acc: 0.48594\n",
            "epoch 4 train acc: 0.67505 dev_acc: 0.48333\n",
            "epoch 5 train acc: 0.67293 dev_acc: 0.48542\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-154-c8f26eb953eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopti\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"UNIK_r_optim.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"epoch\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"train acc:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrect\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"dev_acc:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-154-c8f26eb953eb>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mopti\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mlogit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-147-e74f1df4d7fb>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-147-e74f1df4d7fb>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0ms_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms_unit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt_unit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_out\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresidual\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-147-e74f1df4d7fb>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_channels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0moutput_i\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m             \u001b[0moutput_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_i\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mgpu\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    678\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 680\u001b[0;31m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    681\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z29u9hYxVu3d"
      },
      "source": [
        "def predict(net,dataloader):\n",
        "    net.eval()\n",
        "    predictions = torch.tensor([]).cuda(gpu)\n",
        "    with torch.no_grad():\n",
        "        for i, (x,y) in enumerate(test_loader):\n",
        "            x,y=x.cuda(gpu),y.cuda(gpu)\n",
        "            logit = net(x)\n",
        "            pred = torch.argmax(logit,dim=1)+1\n",
        "            predictions=torch.cat((predictions,pred))\n",
        "    return predictions.cpu().numpy()\n",
        "pred = predict(net,test_loader)\n",
        "print(pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSrbo1knVzGs"
      },
      "source": [
        "output = pd.read_csv(root_path+\"sample.csv\")\n",
        "output['Category']=pred.astype('int')\n",
        "output.head()\n",
        "output.to_csv(\"predictions.csv\",index=None)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}