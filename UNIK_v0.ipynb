{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "UNIK.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7QP48bAaMgR"
      },
      "source": [
        "import math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.utils import shuffle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-QvW9GHkhUnN",
        "outputId": "936e9108-ccb6-4c13-b0f6-d9caf25454b8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s86-bOvUbxCU"
      },
      "source": [
        "def conv_branch_init(conv, branches):\n",
        "    weight = conv.weight\n",
        "    n = weight.size(0)\n",
        "    k1 = weight.size(1)\n",
        "    k2 = weight.size(2)\n",
        "    nn.init.normal_(weight, 0, math.sqrt(2. / (n * k1 * k2 * branches)))\n",
        "    nn.init.constant_(conv.bias, 0)\n",
        "\n",
        "\n",
        "def conv_init(conv):\n",
        "    nn.init.kaiming_normal_(conv.weight, mode='fan_out')\n",
        "    nn.init.constant_(conv.bias, 0)\n",
        "\n",
        "\n",
        "def bn_init(bn, scale):\n",
        "    nn.init.constant_(bn.weight, scale)\n",
        "    nn.init.constant_(bn.bias, 0)\n",
        "\n",
        "\n",
        "# Temporal unit\n",
        "class T_LSU(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=9, stride=1, dilation=1, autopad=True):\n",
        "        super(T_LSU, self).__init__()\n",
        "\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU()\n",
        "        bn_init(self.bn, 1)\n",
        "        if autopad:\n",
        "            pad = int(( kernel_size - 1) * dilation // 2)\n",
        "        else:\n",
        "            pad = 0\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=(kernel_size, 1), padding=(pad, 0),\n",
        "                              stride=(stride, 1), dilation=(dilation, 1))\n",
        "        conv_init(self.conv)\n",
        "\n",
        "    def forward(self, x):\n",
        "        conv_part = self.bn(self.conv(x))\n",
        "        return conv_part\n",
        "\n",
        "# Spatial unit\n",
        "class S_LSU(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, num_joints, num_heads=8, coff_embedding=4, bias=True):\n",
        "        super(S_LSU, self).__init__()\n",
        "        inter_channels = out_channels // coff_embedding\n",
        "        self.inter_c = inter_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.num_heads = num_heads\n",
        "        self.DepM = nn.Parameter(torch.Tensor(num_heads, num_joints, num_joints))\n",
        "        if bias:\n",
        "            self.bias = nn.Parameter(torch.Tensor(num_joints))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "        self.reset_parameters()\n",
        "\n",
        "        # Attention\n",
        "        self.conv_a = nn.ModuleList()\n",
        "        self.conv_b = nn.ModuleList()\n",
        "        self.conv_d = nn.ModuleList()\n",
        "        for i in range(self.num_heads):\n",
        "            self.conv_a.append(nn.Conv2d(in_channels, inter_channels, 1))\n",
        "            self.conv_b.append(nn.Conv2d(in_channels, inter_channels, 1))\n",
        "            self.conv_d.append(nn.Conv2d(in_channels, out_channels, 1))\n",
        "\n",
        "        if in_channels != out_channels:\n",
        "            self.down = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, 1),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "        else:\n",
        "            self.down = lambda x: x\n",
        "\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.soft = nn.Softmax(-2)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                conv_init(m)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                bn_init(m, 1)\n",
        "        bn_init(self.bn, 1e-6)\n",
        "        for i in range(self.num_heads):\n",
        "            conv_branch_init(self.conv_d[i], self.num_heads)\n",
        "\n",
        "\n",
        "    def reset_parameters(self) -> None:\n",
        "        nn.init.kaiming_uniform_(self.DepM, a=math.sqrt(5))\n",
        "        if self.bias is not None:\n",
        "            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.DepM)\n",
        "            bound = 1 / math.sqrt(fan_in)\n",
        "            nn.init.uniform_(self.bias, -bound, bound)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        N, C, T, V = x.size()\n",
        "\n",
        "        W = self.DepM\n",
        "        B = self.bias\n",
        "        y = None\n",
        "        for i in range(self.num_heads):\n",
        "\n",
        "            A1 = self.conv_a[i](x).permute(0, 3, 1, 2).contiguous().view(N, V, self.inter_c * T)\n",
        "            A2 = self.conv_b[i](x).view(N, self.inter_c * T, V)\n",
        "            A1 = self.soft(torch.matmul(A1, A2) / A1.size(-1))  # N tV tV\n",
        "\n",
        "            A1 = W[i] + A1\n",
        "            A2 = x.view(N, C * T, V)\n",
        "            z = self.conv_d[i]((torch.matmul(A2, A1)).view(N, C, T, V))\n",
        "            y = z + y if y is not None else z\n",
        "\n",
        "        y = self.bn(y)\n",
        "        y += self.down(x)\n",
        "        return self.relu(y).view(N, -1, T, V)\n",
        "\n",
        "\n",
        "class ST_block(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, num_joints=20, num_heads=4, dilation=1,stride=1, residual=True,autopad=True):\n",
        "        super(ST_block, self).__init__()\n",
        "        self.s_unit = S_LSU(in_channels, out_channels, num_joints, num_heads)\n",
        "        self.t_unit = T_LSU(out_channels, out_channels, stride=stride, dilation=dilation, autopad=autopad)\n",
        "        self.relu = nn.ReLU()\n",
        "        if not residual:\n",
        "            self.residual = lambda x: 0\n",
        "\n",
        "        elif (in_channels == out_channels) and (stride == 1):\n",
        "            self.residual = lambda x: x\n",
        "\n",
        "        else:\n",
        "            self.residual = T_LSU(in_channels, out_channels, kernel_size=1, stride=stride)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.t_unit(self.s_unit(x)) + self.residual(x)\n",
        "        return self.relu(x)\n",
        "\n",
        "\n",
        "class UNIK(nn.Module):\n",
        "    def __init__(self, num_class=49, num_joints=20, num_heads=3, in_channels=3):\n",
        "        super(UNIK, self).__init__()\n",
        "\n",
        "        self.data_bn = nn.BatchNorm1d(in_channels * num_joints)\n",
        "\n",
        "        self.l1 = ST_block(in_channels, 64, num_joints, residual=False)\n",
        "        self.l2 = ST_block(64, 128, num_joints, num_heads, stride=2)\n",
        "        self.l3 = ST_block(128, 256, num_joints, num_heads, stride=2)\n",
        "        self.l4 = ST_block(256, 128, num_joints, num_heads)\n",
        "        self.fc = nn.Linear(128, num_class)\n",
        "        nn.init.normal_(self.fc.weight, 0, math.sqrt(2. / num_class))\n",
        "        bn_init(self.data_bn, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        N, C, V, T = x.size()\n",
        "        x = x.permute(0,2,1,3).contiguous().view(N,V*C,T)\n",
        "        x = self.data_bn(x)\n",
        "        x = x.view(N,V,C,T).permute(0,2,3,1).contiguous().view(N,C,T,V)\n",
        "\n",
        "        x = self.l1(x)\n",
        "        x = self.l2(x)\n",
        "        x = self.l3(x)\n",
        "        x = self.l4(x)\n",
        "        x=x.view(N,x.size(1),-1)\n",
        "        x = x.mean(-1)\n",
        "\n",
        "        return self.fc(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BaphY4zubAp3"
      },
      "source": [
        "class GCNDataset(Dataset):\n",
        "    def __init__(self,filename,hasLabel=True,aug=False):\n",
        "        self.df = pd.read_csv(filename,header=None)\n",
        "        self.length = len(self.df)\n",
        "        if hasLabel:\n",
        "            #self.df['freq']=1./self.df.groupby(961)[961].transform('count')\n",
        "            #self.df = self.df.sample(frac=1,weights=self.df.freq).reset_index(drop=True)\n",
        "            self.df = self.df.sample(frac=1).reset_index(drop=True)\n",
        "            self.X = torch.tensor(self.df.iloc[:,1:-1].values.astype('float32'))\n",
        "            # N, C, V, T\n",
        "            self.X = self.X.reshape((self.length ,16, 20, 3)).permute(0,3,2,1).contiguous()\n",
        "            if aug:\n",
        "                X1=self.X\n",
        "                X2=self.X\n",
        "                X3=self.X\n",
        "                X1[:,0,:,:] *= -1\n",
        "                X2[:,1,:,:] *= -1\n",
        "                X3[:,2,:,:] *= -1\n",
        "                self.X = torch.cat((X1,X2,X3,self.X),0)\n",
        "            self.labels = self.df.iloc[:,-1].values.astype('int32')-1\n",
        "            self.Y = torch.tensor(self.labels,dtype=torch.long)\n",
        "            #self.Y = np.zeros((labels.size, labels.max()+1))\n",
        "            #self.Y[np.arange(labels.size),labels] = 1\n",
        "        else:\n",
        "            self.X = torch.tensor(self.df.iloc[:,1:].values.astype('float32'))\n",
        "            self.X = self.X.reshape((self.length ,16, 20, 3)).permute(0,3,2,1).contiguous()\n",
        "            #self.Y = np.zeros((self.length,49))\n",
        "            self.Y=torch.tensor(np.zeros(self.length),dtype=torch.long)\n",
        "\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    \n",
        "    def __getitem__(self,index):\n",
        "        x = self.X[index]\n",
        "        y = self.Y[index]\n",
        "        return x, y "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9yV7b5ibLaE"
      },
      "source": [
        "root_path = \"drive/MyDrive/SML/\"\n",
        "bs=64\n",
        "train_set = GCNDataset(filename=root_path+\"training_set.csv\",aug=False)\n",
        "val_set = GCNDataset(filename=root_path+\"val_set.csv\")\n",
        "test_set = GCNDataset(filename=root_path+\"test.csv\",hasLabel=False)\n",
        "train_loader = DataLoader(train_set,batch_size=bs)\n",
        "val_loader = DataLoader(val_set,batch_size=bs)\n",
        "test_loader = DataLoader(test_set,batch_size=bs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CovbgOkq28HV",
        "outputId": "d0645760-3dcc-4dcc-86d1-07c6f59edc53"
      },
      "source": [
        "#!pip install adabound"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting adabound\n",
            "  Downloading adabound-0.0.5-py3-none-any.whl (5.1 kB)\n",
            "Requirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from adabound) (1.9.0+cu102)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.0->adabound) (3.7.4.3)\n",
            "Installing collected packages: adabound\n",
            "Successfully installed adabound-0.0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2fXqnK7c_Yc",
        "outputId": "2bd39920-afbe-4ff3-ca27-79d415bb2995"
      },
      "source": [
        "#import adabound\n",
        "net = UNIK()\n",
        "#net.load_state_dict(torch.load(root_path+'UNIK.pkl'))\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "gpu = 0 #gpu ID\n",
        "net.cuda(gpu)\n",
        "#opti = optim.SGD(net.parameters(),lr=0.01, weight_decay=0.0005)\n",
        "opti = optim.Adam(net.parameters(),lr=0.01,weight_decay=1e-4)\n",
        "#opti.load_state_dict(torch.load(root_path+'UNIK_optim.pkl'))\n",
        "#opti=adabound.AdaBound(net.parameters(),lr=0.1,weight_decay=0.0001, final_lr=0.1)\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkJPZ0n9ds9U",
        "outputId": "c0f0d7be-288b-499f-d416-25e60f0b4aea"
      },
      "source": [
        "def adjust_learning_rate(lr):\n",
        "    for param_group in opti.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "    return lr\n",
        "#adjust_learning_rate(0.001)\n",
        "\n",
        "def accuracy(logit,target):\n",
        "    a=(torch.argmax(logit,dim=1)==target).sum()\n",
        "    return a\n",
        "\n",
        "def evaluate(model, criterion, dataloader, gpu):\n",
        "    model.eval()\n",
        "    acc = 0\n",
        "    count = 0\n",
        "    with torch.no_grad():\n",
        "        for i,(x,y) in enumerate(dataloader):\n",
        "            x,y = x.cuda(gpu), y.cuda(gpu)\n",
        "            logits = model(x)\n",
        "            acc+= accuracy(logits, y)\n",
        "            count += bs\n",
        "\n",
        "    return acc / count\n",
        "\n",
        "def train():\n",
        "    best_acc=0.56\n",
        "    best_epoch = 0\n",
        "    for epoch in range(200):\n",
        "        total = 0\n",
        "        correct = 0\n",
        "        \n",
        "        for i, (x,y) in enumerate(train_loader):\n",
        "            net.train()\n",
        "            opti.zero_grad()\n",
        "            x,y=x.cuda(gpu),y.cuda(gpu)\n",
        "            logit = net(x)\n",
        "            loss = criterion(logit,y)\n",
        "            loss.backward()\n",
        "            opti.step()\n",
        "            correct+=accuracy(logit,y)\n",
        "            total+=bs\n",
        "        dev_acc = evaluate(net, criterion, val_loader, gpu)\n",
        "        \n",
        "        if dev_acc>best_acc:\n",
        "            best_acc=dev_acc\n",
        "            torch.save(net.state_dict(), root_path+'UNIK_best.pkl')\n",
        "            torch.save(opti.state_dict(), root_path+\"UNIK_best_optim.pkl\")\n",
        "        print(\"epoch\",epoch,\"train acc:\",round(float(correct/total),5),\"dev_acc:\",round(float(dev_acc),5))\n",
        "train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0 train acc: 0.21623 dev_acc: 0.27031\n",
            "epoch 1 train acc: 0.31303 dev_acc: 0.32135\n",
            "epoch 2 train acc: 0.35262 dev_acc: 0.36458\n",
            "epoch 3 train acc: 0.38983 dev_acc: 0.39844\n",
            "epoch 4 train acc: 0.4179 dev_acc: 0.41927\n",
            "epoch 5 train acc: 0.44862 dev_acc: 0.4276\n",
            "epoch 6 train acc: 0.46928 dev_acc: 0.44167\n",
            "epoch 7 train acc: 0.49616 dev_acc: 0.46406\n",
            "epoch 8 train acc: 0.52158 dev_acc: 0.4474\n",
            "epoch 9 train acc: 0.54039 dev_acc: 0.45104\n",
            "epoch 10 train acc: 0.56078 dev_acc: 0.47865\n",
            "epoch 11 train acc: 0.58117 dev_acc: 0.47448\n",
            "epoch 12 train acc: 0.60262 dev_acc: 0.46979\n",
            "epoch 13 train acc: 0.62659 dev_acc: 0.47604\n",
            "epoch 14 train acc: 0.65003 dev_acc: 0.46563\n",
            "epoch 15 train acc: 0.66486 dev_acc: 0.46771\n",
            "epoch 16 train acc: 0.68856 dev_acc: 0.46146\n",
            "epoch 17 train acc: 0.69558 dev_acc: 0.45729\n",
            "epoch 18 train acc: 0.71438 dev_acc: 0.46771\n",
            "epoch 19 train acc: 0.73464 dev_acc: 0.47604\n",
            "epoch 20 train acc: 0.74537 dev_acc: 0.47865\n",
            "epoch 21 train acc: 0.76417 dev_acc: 0.45313\n",
            "epoch 22 train acc: 0.78655 dev_acc: 0.46771\n",
            "epoch 23 train acc: 0.79754 dev_acc: 0.46458\n",
            "epoch 24 train acc: 0.8072 dev_acc: 0.46979\n",
            "epoch 25 train acc: 0.83038 dev_acc: 0.48594\n",
            "epoch 26 train acc: 0.83183 dev_acc: 0.46927\n",
            "epoch 27 train acc: 0.82746 dev_acc: 0.47604\n",
            "epoch 28 train acc: 0.8419 dev_acc: 0.5\n",
            "epoch 29 train acc: 0.86454 dev_acc: 0.47917\n",
            "epoch 30 train acc: 0.86043 dev_acc: 0.47969\n",
            "epoch 31 train acc: 0.8697 dev_acc: 0.46615\n",
            "epoch 32 train acc: 0.87023 dev_acc: 0.46927\n",
            "epoch 33 train acc: 0.88414 dev_acc: 0.47917\n",
            "epoch 34 train acc: 0.88149 dev_acc: 0.475\n",
            "epoch 35 train acc: 0.90016 dev_acc: 0.47083\n",
            "epoch 36 train acc: 0.89989 dev_acc: 0.48073\n",
            "epoch 37 train acc: 0.89844 dev_acc: 0.48229\n",
            "epoch 38 train acc: 0.8901 dev_acc: 0.4776\n",
            "epoch 39 train acc: 0.89738 dev_acc: 0.45729\n",
            "epoch 40 train acc: 0.91102 dev_acc: 0.47448\n",
            "epoch 41 train acc: 0.90916 dev_acc: 0.46979\n",
            "epoch 42 train acc: 0.90903 dev_acc: 0.47292\n",
            "epoch 43 train acc: 0.89526 dev_acc: 0.45469\n",
            "epoch 44 train acc: 0.90426 dev_acc: 0.47813\n",
            "epoch 45 train acc: 0.9236 dev_acc: 0.47031\n",
            "epoch 46 train acc: 0.92744 dev_acc: 0.45781\n",
            "epoch 47 train acc: 0.91234 dev_acc: 0.47135\n",
            "epoch 48 train acc: 0.89526 dev_acc: 0.47708\n",
            "epoch 49 train acc: 0.92135 dev_acc: 0.47448\n",
            "epoch 50 train acc: 0.9277 dev_acc: 0.46563\n",
            "epoch 51 train acc: 0.9322 dev_acc: 0.48906\n",
            "epoch 52 train acc: 0.91486 dev_acc: 0.47656\n",
            "epoch 53 train acc: 0.90559 dev_acc: 0.47188\n",
            "epoch 54 train acc: 0.91367 dev_acc: 0.47344\n",
            "epoch 55 train acc: 0.93141 dev_acc: 0.48438\n",
            "epoch 56 train acc: 0.94253 dev_acc: 0.47917\n",
            "epoch 57 train acc: 0.93234 dev_acc: 0.48542\n",
            "epoch 58 train acc: 0.90479 dev_acc: 0.47656\n",
            "epoch 59 train acc: 0.91035 dev_acc: 0.48281\n",
            "epoch 60 train acc: 0.93816 dev_acc: 0.48802\n",
            "epoch 61 train acc: 0.95299 dev_acc: 0.48802\n",
            "epoch 62 train acc: 0.94809 dev_acc: 0.47917\n",
            "epoch 63 train acc: 0.91208 dev_acc: 0.46563\n",
            "epoch 64 train acc: 0.89221 dev_acc: 0.48333\n",
            "epoch 65 train acc: 0.92082 dev_acc: 0.4875\n",
            "epoch 66 train acc: 0.93776 dev_acc: 0.48021\n",
            "epoch 67 train acc: 0.94544 dev_acc: 0.49115\n",
            "epoch 68 train acc: 0.93512 dev_acc: 0.4776\n",
            "epoch 69 train acc: 0.904 dev_acc: 0.48229\n",
            "epoch 70 train acc: 0.91194 dev_acc: 0.47656\n",
            "epoch 71 train acc: 0.9473 dev_acc: 0.49063\n",
            "epoch 72 train acc: 0.94809 dev_acc: 0.47813\n",
            "epoch 73 train acc: 0.92002 dev_acc: 0.4849\n",
            "epoch 74 train acc: 0.92373 dev_acc: 0.47396\n",
            "epoch 75 train acc: 0.93022 dev_acc: 0.47708\n",
            "epoch 76 train acc: 0.93512 dev_acc: 0.48021\n",
            "epoch 77 train acc: 0.92439 dev_acc: 0.4875\n",
            "epoch 78 train acc: 0.94108 dev_acc: 0.49323\n",
            "epoch 79 train acc: 0.9526 dev_acc: 0.47396\n",
            "epoch 80 train acc: 0.91525 dev_acc: 0.46354\n",
            "epoch 81 train acc: 0.91976 dev_acc: 0.47604\n",
            "epoch 82 train acc: 0.93485 dev_acc: 0.47188\n",
            "epoch 83 train acc: 0.94094 dev_acc: 0.4849\n",
            "epoch 84 train acc: 0.94028 dev_acc: 0.48125\n",
            "epoch 85 train acc: 0.92055 dev_acc: 0.47917\n",
            "epoch 86 train acc: 0.92956 dev_acc: 0.46927\n",
            "epoch 87 train acc: 0.93287 dev_acc: 0.48125\n",
            "epoch 88 train acc: 0.94002 dev_acc: 0.48333\n",
            "epoch 89 train acc: 0.94439 dev_acc: 0.47448\n",
            "epoch 90 train acc: 0.94253 dev_acc: 0.48125\n",
            "epoch 91 train acc: 0.93472 dev_acc: 0.45417\n",
            "epoch 92 train acc: 0.91631 dev_acc: 0.47344\n",
            "epoch 93 train acc: 0.93975 dev_acc: 0.48177\n",
            "epoch 94 train acc: 0.93843 dev_acc: 0.47083\n",
            "epoch 95 train acc: 0.91062 dev_acc: 0.47708\n",
            "epoch 96 train acc: 0.92664 dev_acc: 0.46823\n",
            "epoch 97 train acc: 0.95299 dev_acc: 0.48906\n",
            "epoch 98 train acc: 0.95193 dev_acc: 0.4776\n",
            "epoch 99 train acc: 0.93776 dev_acc: 0.4849\n",
            "epoch 100 train acc: 0.92188 dev_acc: 0.47969\n",
            "epoch 101 train acc: 0.91062 dev_acc: 0.4625\n",
            "epoch 102 train acc: 0.93763 dev_acc: 0.48594\n",
            "epoch 103 train acc: 0.95061 dev_acc: 0.45104\n",
            "epoch 104 train acc: 0.93816 dev_acc: 0.45729\n",
            "epoch 105 train acc: 0.92916 dev_acc: 0.47552\n",
            "epoch 106 train acc: 0.93141 dev_acc: 0.46823\n",
            "epoch 107 train acc: 0.92916 dev_acc: 0.47292\n",
            "epoch 108 train acc: 0.9285 dev_acc: 0.4849\n",
            "epoch 109 train acc: 0.95379 dev_acc: 0.49531\n",
            "epoch 110 train acc: 0.94928 dev_acc: 0.49948\n",
            "epoch 111 train acc: 0.93763 dev_acc: 0.45104\n",
            "epoch 112 train acc: 0.91645 dev_acc: 0.47188\n",
            "epoch 113 train acc: 0.92519 dev_acc: 0.47604\n",
            "epoch 114 train acc: 0.95312 dev_acc: 0.49115\n",
            "epoch 115 train acc: 0.94981 dev_acc: 0.47604\n",
            "epoch 116 train acc: 0.94823 dev_acc: 0.47813\n",
            "epoch 117 train acc: 0.93578 dev_acc: 0.49479\n",
            "epoch 118 train acc: 0.92042 dev_acc: 0.48594\n",
            "epoch 119 train acc: 0.93128 dev_acc: 0.47188\n",
            "epoch 120 train acc: 0.93551 dev_acc: 0.48385\n",
            "epoch 121 train acc: 0.94478 dev_acc: 0.48438\n",
            "epoch 122 train acc: 0.94253 dev_acc: 0.47135\n",
            "epoch 123 train acc: 0.94942 dev_acc: 0.48698\n",
            "epoch 124 train acc: 0.94465 dev_acc: 0.47344\n",
            "epoch 125 train acc: 0.92135 dev_acc: 0.46615\n",
            "epoch 126 train acc: 0.91711 dev_acc: 0.48906\n",
            "epoch 127 train acc: 0.93498 dev_acc: 0.47969\n",
            "epoch 128 train acc: 0.95485 dev_acc: 0.48229\n",
            "epoch 129 train acc: 0.95895 dev_acc: 0.48698\n",
            "epoch 130 train acc: 0.93379 dev_acc: 0.49115\n",
            "epoch 131 train acc: 0.91777 dev_acc: 0.46823\n",
            "epoch 132 train acc: 0.92519 dev_acc: 0.47813\n",
            "epoch 133 train acc: 0.94942 dev_acc: 0.4875\n",
            "epoch 134 train acc: 0.94227 dev_acc: 0.48385\n",
            "epoch 135 train acc: 0.95471 dev_acc: 0.49115\n",
            "epoch 136 train acc: 0.95551 dev_acc: 0.48698\n",
            "epoch 137 train acc: 0.9236 dev_acc: 0.47604\n",
            "epoch 138 train acc: 0.9232 dev_acc: 0.46927\n",
            "epoch 139 train acc: 0.93022 dev_acc: 0.49323\n",
            "epoch 140 train acc: 0.93128 dev_acc: 0.48438\n",
            "epoch 141 train acc: 0.94319 dev_acc: 0.46302\n",
            "epoch 142 train acc: 0.96372 dev_acc: 0.49219\n",
            "epoch 143 train acc: 0.94677 dev_acc: 0.48177\n",
            "epoch 144 train acc: 0.93287 dev_acc: 0.46927\n",
            "epoch 145 train acc: 0.92611 dev_acc: 0.48229\n",
            "epoch 146 train acc: 0.92254 dev_acc: 0.49219\n",
            "epoch 147 train acc: 0.94213 dev_acc: 0.47656\n",
            "epoch 148 train acc: 0.94319 dev_acc: 0.5\n",
            "epoch 149 train acc: 0.95458 dev_acc: 0.4974\n",
            "epoch 150 train acc: 0.94955 dev_acc: 0.48958\n",
            "epoch 151 train acc: 0.93512 dev_acc: 0.48854\n",
            "epoch 152 train acc: 0.93737 dev_acc: 0.48021\n",
            "epoch 153 train acc: 0.94637 dev_acc: 0.48021\n",
            "epoch 154 train acc: 0.94227 dev_acc: 0.47813\n",
            "epoch 155 train acc: 0.92519 dev_acc: 0.47448\n",
            "epoch 156 train acc: 0.92042 dev_acc: 0.47396\n",
            "epoch 157 train acc: 0.9334 dev_acc: 0.49948\n",
            "epoch 158 train acc: 0.96094 dev_acc: 0.47448\n",
            "epoch 159 train acc: 0.95895 dev_acc: 0.5026\n",
            "epoch 160 train acc: 0.95551 dev_acc: 0.47396\n",
            "epoch 161 train acc: 0.92572 dev_acc: 0.4849\n",
            "epoch 162 train acc: 0.8844 dev_acc: 0.48385\n",
            "epoch 163 train acc: 0.93882 dev_acc: 0.47969\n",
            "epoch 164 train acc: 0.96359 dev_acc: 0.48281\n",
            "epoch 165 train acc: 0.97471 dev_acc: 0.48333\n",
            "epoch 166 train acc: 0.96796 dev_acc: 0.49063\n",
            "epoch 167 train acc: 0.91909 dev_acc: 0.48594\n",
            "epoch 168 train acc: 0.88242 dev_acc: 0.48177\n",
            "epoch 169 train acc: 0.92135 dev_acc: 0.48594\n",
            "epoch 170 train acc: 0.94611 dev_acc: 0.47865\n",
            "epoch 171 train acc: 0.96292 dev_acc: 0.48958\n",
            "epoch 172 train acc: 0.96928 dev_acc: 0.48333\n",
            "epoch 173 train acc: 0.95829 dev_acc: 0.45469\n",
            "epoch 174 train acc: 0.91247 dev_acc: 0.45781\n",
            "epoch 175 train acc: 0.89327 dev_acc: 0.46302\n",
            "epoch 176 train acc: 0.94399 dev_acc: 0.48906\n",
            "epoch 177 train acc: 0.95074 dev_acc: 0.49896\n",
            "epoch 178 train acc: 0.94876 dev_acc: 0.48542\n",
            "epoch 179 train acc: 0.94028 dev_acc: 0.4901\n",
            "epoch 180 train acc: 0.94717 dev_acc: 0.49271\n",
            "epoch 181 train acc: 0.95127 dev_acc: 0.47813\n",
            "epoch 182 train acc: 0.94981 dev_acc: 0.4875\n",
            "epoch 183 train acc: 0.94213 dev_acc: 0.47552\n",
            "epoch 184 train acc: 0.90334 dev_acc: 0.47135\n",
            "epoch 185 train acc: 0.92969 dev_acc: 0.50417\n",
            "epoch 186 train acc: 0.95352 dev_acc: 0.48229\n",
            "epoch 187 train acc: 0.9718 dev_acc: 0.47969\n",
            "epoch 188 train acc: 0.97842 dev_acc: 0.475\n",
            "epoch 189 train acc: 0.96319 dev_acc: 0.46823\n",
            "epoch 190 train acc: 0.88665 dev_acc: 0.49583\n",
            "epoch 191 train acc: 0.8897 dev_acc: 0.48385\n",
            "epoch 192 train acc: 0.95855 dev_acc: 0.51198\n",
            "epoch 193 train acc: 0.98001 dev_acc: 0.49479\n",
            "epoch 194 train acc: 0.98808 dev_acc: 0.50625\n",
            "epoch 195 train acc: 0.98782 dev_acc: 0.50365\n",
            "epoch 196 train acc: 0.97391 dev_acc: 0.4651\n",
            "epoch 197 train acc: 0.83038 dev_acc: 0.4599\n",
            "epoch 198 train acc: 0.85898 dev_acc: 0.49792\n",
            "epoch 199 train acc: 0.95008 dev_acc: 0.47656\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EWa0t0MbdW9"
      },
      "source": [
        "torch.save(net.state_dict(), root_path+'UNIK.pkl')\n",
        "torch.save(opti.state_dict(), root_path+\"UNIK_optim.pkl\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35vWxYnGxncv",
        "outputId": "cc41e06f-fd42-49b5-9e67-cdc12e58df0a"
      },
      "source": [
        "def predict(net,dataloader):\n",
        "    net.eval()\n",
        "    predictions = torch.tensor([]).cuda(gpu)\n",
        "    with torch.no_grad():\n",
        "        for i, (x,y) in enumerate(test_loader):\n",
        "            x,y=x.cuda(gpu),y.cuda(gpu)\n",
        "            logit = net(x)\n",
        "            pred = torch.argmax(logit,dim=1)+1\n",
        "            predictions=torch.cat((predictions,pred))\n",
        "    return predictions.cpu().numpy()\n",
        "pred = predict(net,test_loader)\n",
        "print(pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[22. 11.  9. ... 32. 13. 30.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tiSrPYEvx2YX"
      },
      "source": [
        "output = pd.read_csv(root_path+\"sample.csv\")\n",
        "output['Category']=pred.astype('int')\n",
        "output.head()\n",
        "output.to_csv(\"predictions.csv\",index=None)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}