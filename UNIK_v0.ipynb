{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "UNIK.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7QP48bAaMgR"
      },
      "source": [
        "import math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.utils import shuffle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-QvW9GHkhUnN",
        "outputId": "616f890d-79f8-4421-b7e7-680804a291f3"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s86-bOvUbxCU"
      },
      "source": [
        "def conv_branch_init(conv, branches):\n",
        "    weight = conv.weight\n",
        "    n = weight.size(0)\n",
        "    k1 = weight.size(1)\n",
        "    k2 = weight.size(2)\n",
        "    nn.init.normal_(weight, 0, math.sqrt(2. / (n * k1 * k2 * branches)))\n",
        "    nn.init.constant_(conv.bias, 0)\n",
        "\n",
        "\n",
        "def conv_init(conv):\n",
        "    nn.init.kaiming_normal_(conv.weight, mode='fan_out')\n",
        "    nn.init.constant_(conv.bias, 0)\n",
        "\n",
        "\n",
        "def bn_init(bn, scale):\n",
        "    nn.init.constant_(bn.weight, scale)\n",
        "    nn.init.constant_(bn.bias, 0)\n",
        "\n",
        "class UnfoldTemporalWindows(nn.Module):\n",
        "    def __init__(self, window_size, window_stride=1, window_dilation=1):\n",
        "        super().__init__()\n",
        "        self.window_size = window_size\n",
        "        self.window_stride = window_stride\n",
        "        self.window_dilation = window_dilation\n",
        "\n",
        "        self.padding = (window_size + (window_size-1) * (window_dilation-1) - 1) // 2\n",
        "        self.unfold = nn.Unfold(kernel_size=(self.window_size, 1),\n",
        "                                dilation=(self.window_dilation, 1),\n",
        "                                stride=(self.window_stride, 1),\n",
        "                                padding=(self.padding, 0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input shape: (N,C,T,V), out: (N,C,T,V*tau)\n",
        "        N, C, T, V = x.shape\n",
        "        x = self.unfold(x)\n",
        "        x = x.view(N, C, self.window_size, -1, V).permute(0,1,3,2,4).contiguous()\n",
        "        x = x.view(N, C, -1, self.window_size * V)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Temporal unit\n",
        "class T_LSU(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, dilation=1, autopad=True):\n",
        "        super(T_LSU, self).__init__()\n",
        "        if autopad:\n",
        "            pad = int(( kernel_size - 1) * dilation // 2)\n",
        "        else:\n",
        "            pad = 0\n",
        "\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=(kernel_size, 1), padding=(pad, 0),\n",
        "                              stride=(stride, 1), dilation=(dilation, 1))\n",
        "   \n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU()\n",
        "        conv_init(self.conv)\n",
        "        bn_init(self.bn, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.bn(self.conv(x))\n",
        "        return x\n",
        "\n",
        "# Spatial unit\n",
        "class S_LSU(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, num_joints, tau=1, num_heads=8, coff_embedding=4, bias=True):\n",
        "        super(S_LSU, self).__init__()\n",
        "        inter_channels = out_channels // coff_embedding\n",
        "        self.inter_c = inter_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.tau = tau\n",
        "        self.num_heads = num_heads\n",
        "        self.DepM = nn.Parameter(torch.Tensor(num_heads, num_joints*tau, num_joints*tau))\n",
        "        if bias:\n",
        "            self.bias = nn.Parameter(torch.Tensor(num_joints*tau))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "        self.reset_parameters()\n",
        "        \n",
        "        # Temporal window        \n",
        "        if tau != 1:\n",
        "            self.tw = UnfoldTemporalWindows(window_size=tau, window_stride=1, window_dilation=1)\n",
        "            self.out_conv = nn.Conv3d(out_channels, out_channels, kernel_size=(1, tau, 1))\n",
        "            self.out_bn = nn.BatchNorm2d(out_channels)\n",
        "        # Attention\n",
        "        self.conv_a = nn.ModuleList()\n",
        "        self.conv_b = nn.ModuleList()\n",
        "        self.conv_d = nn.ModuleList()\n",
        "        for i in range(self.num_heads):\n",
        "            self.conv_a.append(nn.Conv2d(in_channels, inter_channels, 1))\n",
        "            self.conv_b.append(nn.Conv2d(in_channels, inter_channels, 1))\n",
        "            self.conv_d.append(nn.Conv2d(in_channels, out_channels, 1))\n",
        "\n",
        "        if in_channels != out_channels:\n",
        "            self.down = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, 1),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "        else:\n",
        "            self.down = lambda x: x\n",
        "\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.soft = nn.Softmax(-2)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                conv_init(m)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                bn_init(m, 1)\n",
        "        bn_init(self.bn, 1e-6)\n",
        "        for i in range(self.num_heads):\n",
        "            conv_branch_init(self.conv_d[i], self.num_heads)\n",
        "            \n",
        "            \n",
        "    def reset_parameters(self) -> None:\n",
        "        nn.init.kaiming_uniform_(self.DepM, a=math.sqrt(5))\n",
        "        if self.bias is not None:\n",
        "            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.DepM)\n",
        "            bound = 1 / math.sqrt(fan_in)\n",
        "            nn.init.uniform_(self.bias, -bound, bound)\n",
        "            \n",
        "            \n",
        "    def forward(self, x):\n",
        "        if self.tau != 1:\n",
        "            x = self.tw(x)\n",
        "        N, C, T, V = x.size()\n",
        "     \n",
        "        W = self.DepM\n",
        "        B = self.bias\n",
        "        y = None\n",
        "        for i in range(self.num_heads):\n",
        "           \n",
        "            A1 = self.conv_a[i](x).permute(0, 3, 1, 2).contiguous().view(N, V, self.inter_c * T)\n",
        "            A2 = self.conv_b[i](x).view(N, self.inter_c * T, V)\n",
        "            A1 = self.soft(torch.matmul(A1, A2) / A1.size(-1))  # N tV tV\n",
        "            \n",
        "            A1 = W[i] + A1\n",
        "            A2 = x.view(N, C * T, V)\n",
        "            z = self.conv_d[i]((torch.matmul(A2, A1)).view(N, C, T, V))\n",
        "            y = z + y if y is not None else z\n",
        "\n",
        "        y = self.bn(y)\n",
        "        y += self.down(x)\n",
        "        \n",
        "        if self.tau == 1:\n",
        "            return self.relu(y).view(N, -1, T, V)\n",
        "        else:\n",
        "            y = self.relu(y)\n",
        "            y = y.view(N, self.out_channels, -1, self.tau, V // self.tau)\n",
        "            y = self.out_conv(y).squeeze(dim=3)\n",
        "            y = self.out_bn(y)\n",
        "            return y\n",
        "\n",
        "\n",
        "\n",
        "class ST_block(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, num_joints=20, tau=1, num_heads=4, stride=1, dilation=1, autopad=True, residual=True):\n",
        "        super(ST_block, self).__init__()\n",
        "        self.s_unit = S_LSU(in_channels, out_channels, num_joints, tau, num_heads)\n",
        "        self.t_unit = T_LSU(out_channels, out_channels, stride=stride, dilation=dilation, autopad=autopad)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.pad = 0\n",
        "        if not autopad:\n",
        "            self.pad = (tau - 1) * dilation // 2\n",
        "\n",
        "        if not residual:\n",
        "            self.residual = lambda x: 0\n",
        "\n",
        "        elif (in_channels == out_channels) and (stride == 1):\n",
        "            self.residual = lambda x: x\n",
        "\n",
        "        else:\n",
        "            self.residual = T_LSU(in_channels, out_channels, kernel_size=1, stride=stride)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.t_unit(self.s_unit(x)) + self.residual(x[:, :, self.pad : x.shape[2] - self.pad, :])\n",
        "        return self.relu(x)\n",
        "\n",
        "\n",
        "class UNIK(nn.Module):\n",
        "    def __init__(self, num_class=49, num_joints=20, tau=1, num_heads=4, in_channels=3):\n",
        "        super(UNIK, self).__init__()\n",
        "\n",
        "        \n",
        "        self.tau = tau\n",
        "        self.data_bn = nn.BatchNorm1d(in_channels * num_joints)\n",
        "\n",
        "        self.l1 = ST_block(in_channels, 64, num_joints, tau, residual=False)\n",
        "        self.l2 = ST_block(64, 128, num_joints, tau, num_heads, stride=2)\n",
        "        self.l3 = ST_block(128, 256, num_joints, tau, num_heads, stride=2)\n",
        "        self.fc = nn.Linear(256, num_class)\n",
        "        nn.init.normal_(self.fc.weight, 0, math.sqrt(2. / num_class))\n",
        "        bn_init(self.data_bn, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        N, C, V, T = x.size()\n",
        "        x = x.permute(0,2,1,3).contiguous().view(N,V*C,T)\n",
        "        x = self.data_bn(x)\n",
        "        x = x.view(N,V,C,T).permute(0,2,3,1).contiguous().view(N,C,T,V)\n",
        "\n",
        "        x = self.l1(x)\n",
        "        x = self.l2(x)\n",
        "        x = self.l3(x)\n",
        "\n",
        "        c_new = x.size(1)\n",
        "        x=x.view(N,c_new,-1)\n",
        "        x = x.mean(2)\n",
        "\n",
        "        return self.fc(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BaphY4zubAp3"
      },
      "source": [
        "class GCNDataset(Dataset):\n",
        "    def __init__(self,filename,hasLabel=True,aug=False):\n",
        "        self.df = pd.read_csv(filename,header=None)\n",
        "        self.length = len(self.df)\n",
        "        if hasLabel:\n",
        "            #self.df['freq']=1./self.df.groupby(961)[961].transform('count')\n",
        "            #self.df = self.df.sample(frac=1,weights=self.df.freq).reset_index(drop=True)\n",
        "            self.df = self.df.sample(frac=1).reset_index(drop=True)\n",
        "            self.X = torch.tensor(self.df.iloc[:,1:-1].values.astype('float32'))\n",
        "            # N, C, V, T\n",
        "            self.X = self.X.reshape((self.length ,16, 20, 3)).permute(0,3,2,1).contiguous()\n",
        "            if aug:\n",
        "                X1=self.X\n",
        "                X2=self.X\n",
        "                X3=self.X\n",
        "                X1[:,0,:,:] *= -1\n",
        "                X2[:,1,:,:] *= -1\n",
        "                X3[:,2,:,:] *= -1\n",
        "                self.X = torch.cat((X1,X2,X3,self.X),0)\n",
        "            self.labels = self.df.iloc[:,-1].values.astype('int32')-1\n",
        "            self.Y = torch.tensor(self.labels,dtype=torch.long)\n",
        "            #self.Y = np.zeros((labels.size, labels.max()+1))\n",
        "            #self.Y[np.arange(labels.size),labels] = 1\n",
        "        else:\n",
        "            self.X = torch.tensor(self.df.iloc[:,1:].values.astype('float32'))\n",
        "            self.X = self.X.reshape((self.length ,16, 20, 3)).permute(0,3,2,1).contiguous()\n",
        "            #self.Y = np.zeros((self.length,49))\n",
        "            self.Y=torch.tensor(np.zeros(self.length),dtype=torch.long)\n",
        "\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    \n",
        "    def __getitem__(self,index):\n",
        "        x = self.X[index]\n",
        "        y = self.Y[index]\n",
        "        return x, y "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9yV7b5ibLaE"
      },
      "source": [
        "root_path = \"drive/MyDrive/SML/\"\n",
        "bs=64\n",
        "train_set = GCNDataset(filename=root_path+\"training_set.csv\",aug=False)\n",
        "val_set = GCNDataset(filename=root_path+\"val_set.csv\")\n",
        "test_set = GCNDataset(filename=root_path+\"test.csv\",hasLabel=False)\n",
        "train_loader = DataLoader(train_set,batch_size=bs)\n",
        "val_loader = DataLoader(val_set,batch_size=bs)\n",
        "test_loader = DataLoader(test_set,batch_size=bs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CovbgOkq28HV",
        "outputId": "d0645760-3dcc-4dcc-86d1-07c6f59edc53"
      },
      "source": [
        "#!pip install adabound"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting adabound\n",
            "  Downloading adabound-0.0.5-py3-none-any.whl (5.1 kB)\n",
            "Requirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from adabound) (1.9.0+cu102)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.0->adabound) (3.7.4.3)\n",
            "Installing collected packages: adabound\n",
            "Successfully installed adabound-0.0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2fXqnK7c_Yc",
        "outputId": "f91995bd-afb3-431b-db47-43dafa6987d1"
      },
      "source": [
        "#import adabound\n",
        "net = UNIK()\n",
        "#net.load_state_dict(torch.load(root_path+'UNIK_best.pkl'))\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "gpu = 0 #gpu ID\n",
        "net.cuda(gpu)\n",
        "#opti = optim.SGD(net.parameters(),lr=0.01)\n",
        "opti = optim.Adam(net.parameters(),lr=0.01)#,weight_decay=0.0001)\n",
        "#opti.load_state_dict(torch.load(root_path+'UNIK_best_optim.pkl'))\n",
        "#opti=adabound.AdaBound(net.parameters(),lr=0.1,weight_decay=0.0001, final_lr=0.1)\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nkJPZ0n9ds9U",
        "outputId": "7fcdb10a-e5f3-48da-d040-f644be0e1d6c"
      },
      "source": [
        "def adjust_learning_rate(lr):\n",
        "    for param_group in opti.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "    return lr\n",
        "\n",
        "\n",
        "def accuracy(logit,target):\n",
        "    a=(torch.argmax(logit,dim=1)==target).sum()\n",
        "    return a\n",
        "\n",
        "def evaluate(model, criterion, dataloader, gpu):\n",
        "    model.eval()\n",
        "    acc = 0\n",
        "    count = 0\n",
        "    with torch.no_grad():\n",
        "        for i,(x,y) in enumerate(dataloader):\n",
        "            x,y = x.cuda(gpu), y.cuda(gpu)\n",
        "            logits = model(x)\n",
        "            acc+= accuracy(logits, y)\n",
        "            count += bs\n",
        "\n",
        "    return acc / count\n",
        "\n",
        "def train():\n",
        "    best_acc=0.56\n",
        "    best_epoch = 0\n",
        "    for epoch in range(200):\n",
        "        \"\"\"\n",
        "        if epoch==0:\n",
        "            adjust_learning_rate(0.1)\n",
        "        elif epoch==10:\n",
        "            adjust_learning_rate(0.01)\n",
        "        elif epoch==50:\n",
        "            adjust_learning_rate(0.001)\n",
        "        \"\"\"\n",
        "        total = 0\n",
        "        correct = 0\n",
        "        \n",
        "        for i, (x,y) in enumerate(train_loader):\n",
        "            net.train()\n",
        "            opti.zero_grad()\n",
        "            x,y=x.cuda(gpu),y.cuda(gpu)\n",
        "            logit = net(x)\n",
        "            loss = criterion(logit,y)\n",
        "            loss.backward()\n",
        "            opti.step()\n",
        "            correct+=accuracy(logit,y)\n",
        "            total+=bs\n",
        "        dev_acc = evaluate(net, criterion, val_loader, gpu)\n",
        "        \n",
        "        if dev_acc>best_acc:\n",
        "            best_acc=dev_acc\n",
        "            torch.save(net.state_dict(), root_path+'UNIK_best.pkl')\n",
        "            torch.save(opti.state_dict(), root_path+\"UNIK_best_optim.pkl\")\n",
        "        print(\"epoch\",epoch,\"train acc:\",round(float(correct/total),5),\"dev_acc:\",round(float(dev_acc),5))\n",
        "train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0 train acc: 0.61719 dev_acc: 0.42604\n",
            "epoch 1 train acc: 0.67082 dev_acc: 0.4151\n",
            "epoch 2 train acc: 0.70763 dev_acc: 0.44427\n",
            "epoch 3 train acc: 0.73769 dev_acc: 0.43542\n",
            "epoch 4 train acc: 0.77238 dev_acc: 0.45521\n",
            "epoch 5 train acc: 0.81329 dev_acc: 0.46094\n",
            "epoch 6 train acc: 0.84137 dev_acc: 0.47604\n",
            "epoch 7 train acc: 0.87142 dev_acc: 0.48438\n",
            "epoch 8 train acc: 0.91115 dev_acc: 0.47396\n",
            "epoch 9 train acc: 0.93154 dev_acc: 0.46667\n",
            "epoch 10 train acc: 0.94041 dev_acc: 0.48125\n",
            "epoch 11 train acc: 0.95339 dev_acc: 0.47292\n",
            "epoch 12 train acc: 0.95101 dev_acc: 0.47656\n",
            "epoch 13 train acc: 0.95458 dev_acc: 0.475\n",
            "epoch 14 train acc: 0.95855 dev_acc: 0.48698\n",
            "epoch 15 train acc: 0.9657 dev_acc: 0.48177\n",
            "epoch 16 train acc: 0.96239 dev_acc: 0.49115\n",
            "epoch 17 train acc: 0.96822 dev_acc: 0.49375\n",
            "epoch 18 train acc: 0.97087 dev_acc: 0.49375\n",
            "epoch 19 train acc: 0.97259 dev_acc: 0.5\n",
            "epoch 20 train acc: 0.98146 dev_acc: 0.4974\n",
            "epoch 21 train acc: 0.98543 dev_acc: 0.49792\n",
            "epoch 22 train acc: 0.99192 dev_acc: 0.50417\n",
            "epoch 23 train acc: 0.99139 dev_acc: 0.49948\n",
            "epoch 24 train acc: 0.99073 dev_acc: 0.4974\n",
            "epoch 25 train acc: 0.99086 dev_acc: 0.50938\n",
            "epoch 26 train acc: 0.98106 dev_acc: 0.48385\n",
            "epoch 27 train acc: 0.91631 dev_acc: 0.42344\n",
            "epoch 28 train acc: 0.92055 dev_acc: 0.47917\n",
            "epoch 29 train acc: 0.97431 dev_acc: 0.49271\n",
            "epoch 30 train acc: 0.9849 dev_acc: 0.49792\n",
            "epoch 31 train acc: 0.99285 dev_acc: 0.51042\n",
            "epoch 32 train acc: 0.99457 dev_acc: 0.51198\n",
            "epoch 33 train acc: 0.99457 dev_acc: 0.51458\n",
            "epoch 34 train acc: 0.99457 dev_acc: 0.51458\n",
            "epoch 35 train acc: 0.99457 dev_acc: 0.51354\n",
            "epoch 36 train acc: 0.99457 dev_acc: 0.51354\n",
            "epoch 37 train acc: 0.99457 dev_acc: 0.51302\n",
            "epoch 38 train acc: 0.99457 dev_acc: 0.5125\n",
            "epoch 39 train acc: 0.99457 dev_acc: 0.51354\n",
            "epoch 40 train acc: 0.99457 dev_acc: 0.51458\n",
            "epoch 41 train acc: 0.99457 dev_acc: 0.51406\n",
            "epoch 42 train acc: 0.99457 dev_acc: 0.51406\n",
            "epoch 43 train acc: 0.99457 dev_acc: 0.51302\n",
            "epoch 44 train acc: 0.99457 dev_acc: 0.51302\n",
            "epoch 45 train acc: 0.99457 dev_acc: 0.51198\n",
            "epoch 46 train acc: 0.99457 dev_acc: 0.51198\n",
            "epoch 47 train acc: 0.99457 dev_acc: 0.51198\n",
            "epoch 48 train acc: 0.99457 dev_acc: 0.51146\n",
            "epoch 49 train acc: 0.99457 dev_acc: 0.51146\n",
            "epoch 50 train acc: 0.99457 dev_acc: 0.5125\n",
            "epoch 51 train acc: 0.99457 dev_acc: 0.51302\n",
            "epoch 52 train acc: 0.99457 dev_acc: 0.51354\n",
            "epoch 53 train acc: 0.99457 dev_acc: 0.51406\n",
            "epoch 54 train acc: 0.99457 dev_acc: 0.51354\n",
            "epoch 55 train acc: 0.99457 dev_acc: 0.51354\n",
            "epoch 56 train acc: 0.99457 dev_acc: 0.5125\n",
            "epoch 57 train acc: 0.99457 dev_acc: 0.51302\n",
            "epoch 58 train acc: 0.99457 dev_acc: 0.51302\n",
            "epoch 59 train acc: 0.99457 dev_acc: 0.51302\n",
            "epoch 60 train acc: 0.99457 dev_acc: 0.51198\n",
            "epoch 61 train acc: 0.99457 dev_acc: 0.51198\n",
            "epoch 62 train acc: 0.99457 dev_acc: 0.51198\n",
            "epoch 63 train acc: 0.99457 dev_acc: 0.51302\n",
            "epoch 64 train acc: 0.99457 dev_acc: 0.5125\n",
            "epoch 65 train acc: 0.99457 dev_acc: 0.51302\n",
            "epoch 66 train acc: 0.99457 dev_acc: 0.51198\n",
            "epoch 67 train acc: 0.99457 dev_acc: 0.51198\n",
            "epoch 68 train acc: 0.99457 dev_acc: 0.51198\n",
            "epoch 69 train acc: 0.99457 dev_acc: 0.51146\n",
            "epoch 70 train acc: 0.99457 dev_acc: 0.51146\n",
            "epoch 71 train acc: 0.99457 dev_acc: 0.51146\n",
            "epoch 72 train acc: 0.99457 dev_acc: 0.51198\n",
            "epoch 73 train acc: 0.99457 dev_acc: 0.51302\n",
            "epoch 74 train acc: 0.99457 dev_acc: 0.51302\n",
            "epoch 75 train acc: 0.99457 dev_acc: 0.5125\n",
            "epoch 76 train acc: 0.99457 dev_acc: 0.51302\n",
            "epoch 77 train acc: 0.99457 dev_acc: 0.51354\n",
            "epoch 78 train acc: 0.99457 dev_acc: 0.51302\n",
            "epoch 79 train acc: 0.99457 dev_acc: 0.51354\n",
            "epoch 80 train acc: 0.99457 dev_acc: 0.51302\n",
            "epoch 81 train acc: 0.99457 dev_acc: 0.51302\n",
            "epoch 82 train acc: 0.99457 dev_acc: 0.51302\n",
            "epoch 83 train acc: 0.99457 dev_acc: 0.51302\n",
            "epoch 84 train acc: 0.99457 dev_acc: 0.51302\n",
            "epoch 85 train acc: 0.99457 dev_acc: 0.51354\n",
            "epoch 86 train acc: 0.99457 dev_acc: 0.51354\n",
            "epoch 87 train acc: 0.99457 dev_acc: 0.51406\n",
            "epoch 88 train acc: 0.99457 dev_acc: 0.51406\n",
            "epoch 89 train acc: 0.99457 dev_acc: 0.51302\n",
            "epoch 90 train acc: 0.99457 dev_acc: 0.51302\n",
            "epoch 91 train acc: 0.99457 dev_acc: 0.51302\n",
            "epoch 92 train acc: 0.99457 dev_acc: 0.51354\n",
            "epoch 93 train acc: 0.99457 dev_acc: 0.51354\n",
            "epoch 94 train acc: 0.99457 dev_acc: 0.51354\n",
            "epoch 95 train acc: 0.99457 dev_acc: 0.51406\n",
            "epoch 96 train acc: 0.99457 dev_acc: 0.51406\n",
            "epoch 97 train acc: 0.99457 dev_acc: 0.51458\n",
            "epoch 98 train acc: 0.99457 dev_acc: 0.51458\n",
            "epoch 99 train acc: 0.99457 dev_acc: 0.51458\n",
            "epoch 100 train acc: 0.99457 dev_acc: 0.5151\n",
            "epoch 101 train acc: 0.99457 dev_acc: 0.5151\n",
            "epoch 102 train acc: 0.99457 dev_acc: 0.5151\n",
            "epoch 103 train acc: 0.99457 dev_acc: 0.5151\n",
            "epoch 104 train acc: 0.99457 dev_acc: 0.51562\n",
            "epoch 105 train acc: 0.99457 dev_acc: 0.51562\n",
            "epoch 106 train acc: 0.99457 dev_acc: 0.51562\n",
            "epoch 107 train acc: 0.99457 dev_acc: 0.5151\n",
            "epoch 108 train acc: 0.99457 dev_acc: 0.51562\n",
            "epoch 109 train acc: 0.99457 dev_acc: 0.51562\n",
            "epoch 110 train acc: 0.99457 dev_acc: 0.51562\n",
            "epoch 111 train acc: 0.99457 dev_acc: 0.51562\n",
            "epoch 112 train acc: 0.99457 dev_acc: 0.51562\n",
            "epoch 113 train acc: 0.99457 dev_acc: 0.5151\n",
            "epoch 114 train acc: 0.99457 dev_acc: 0.5151\n",
            "epoch 115 train acc: 0.99457 dev_acc: 0.51562\n",
            "epoch 116 train acc: 0.99457 dev_acc: 0.51562\n",
            "epoch 117 train acc: 0.99457 dev_acc: 0.51615\n",
            "epoch 118 train acc: 0.99457 dev_acc: 0.51615\n",
            "epoch 119 train acc: 0.99457 dev_acc: 0.51615\n",
            "epoch 120 train acc: 0.99457 dev_acc: 0.51562\n",
            "epoch 121 train acc: 0.99457 dev_acc: 0.51458\n",
            "epoch 122 train acc: 0.99457 dev_acc: 0.51458\n",
            "epoch 123 train acc: 0.99457 dev_acc: 0.51458\n",
            "epoch 124 train acc: 0.99457 dev_acc: 0.51458\n",
            "epoch 125 train acc: 0.99457 dev_acc: 0.51458\n",
            "epoch 126 train acc: 0.99457 dev_acc: 0.5151\n",
            "epoch 127 train acc: 0.99457 dev_acc: 0.5151\n",
            "epoch 128 train acc: 0.99457 dev_acc: 0.5151\n",
            "epoch 129 train acc: 0.99457 dev_acc: 0.51458\n",
            "epoch 130 train acc: 0.99457 dev_acc: 0.51458\n",
            "epoch 131 train acc: 0.99457 dev_acc: 0.51458\n",
            "epoch 132 train acc: 0.99457 dev_acc: 0.51458\n",
            "epoch 133 train acc: 0.99457 dev_acc: 0.51458\n",
            "epoch 134 train acc: 0.99457 dev_acc: 0.51406\n",
            "epoch 135 train acc: 0.99457 dev_acc: 0.51406\n",
            "epoch 136 train acc: 0.99457 dev_acc: 0.51406\n",
            "epoch 137 train acc: 0.99457 dev_acc: 0.51406\n",
            "epoch 138 train acc: 0.99457 dev_acc: 0.51406\n",
            "epoch 139 train acc: 0.99457 dev_acc: 0.51406\n",
            "epoch 140 train acc: 0.99457 dev_acc: 0.51406\n",
            "epoch 141 train acc: 0.99457 dev_acc: 0.51406\n",
            "epoch 142 train acc: 0.99457 dev_acc: 0.51406\n",
            "epoch 143 train acc: 0.99457 dev_acc: 0.51406\n",
            "epoch 144 train acc: 0.99457 dev_acc: 0.51406\n",
            "epoch 145 train acc: 0.99457 dev_acc: 0.51406\n",
            "epoch 146 train acc: 0.99457 dev_acc: 0.51458\n",
            "epoch 147 train acc: 0.99457 dev_acc: 0.51406\n",
            "epoch 148 train acc: 0.99457 dev_acc: 0.51458\n",
            "epoch 149 train acc: 0.99457 dev_acc: 0.51406\n",
            "epoch 150 train acc: 0.99457 dev_acc: 0.51406\n",
            "epoch 151 train acc: 0.99457 dev_acc: 0.51406\n",
            "epoch 152 train acc: 0.99457 dev_acc: 0.51458\n",
            "epoch 153 train acc: 0.99457 dev_acc: 0.51458\n",
            "epoch 154 train acc: 0.99457 dev_acc: 0.5151\n",
            "epoch 155 train acc: 0.99457 dev_acc: 0.51458\n",
            "epoch 156 train acc: 0.99457 dev_acc: 0.51458\n",
            "epoch 157 train acc: 0.99457 dev_acc: 0.51406\n",
            "epoch 158 train acc: 0.99457 dev_acc: 0.51406\n",
            "epoch 159 train acc: 0.99457 dev_acc: 0.51458\n",
            "epoch 160 train acc: 0.99457 dev_acc: 0.51458\n",
            "epoch 161 train acc: 0.99457 dev_acc: 0.51406\n",
            "epoch 162 train acc: 0.99457 dev_acc: 0.51458\n",
            "epoch 163 train acc: 0.99457 dev_acc: 0.5151\n",
            "epoch 164 train acc: 0.99457 dev_acc: 0.5151\n",
            "epoch 165 train acc: 0.99457 dev_acc: 0.5151\n",
            "epoch 166 train acc: 0.99457 dev_acc: 0.51458\n",
            "epoch 167 train acc: 0.99457 dev_acc: 0.51458\n",
            "epoch 168 train acc: 0.99457 dev_acc: 0.5151\n",
            "epoch 169 train acc: 0.99457 dev_acc: 0.5151\n",
            "epoch 170 train acc: 0.99457 dev_acc: 0.5151\n",
            "epoch 171 train acc: 0.99457 dev_acc: 0.51458\n",
            "epoch 172 train acc: 0.99457 dev_acc: 0.51406\n",
            "epoch 173 train acc: 0.99457 dev_acc: 0.51354\n",
            "epoch 174 train acc: 0.99457 dev_acc: 0.51302\n",
            "epoch 175 train acc: 0.99457 dev_acc: 0.51406\n",
            "epoch 176 train acc: 0.99457 dev_acc: 0.51406\n",
            "epoch 177 train acc: 0.99457 dev_acc: 0.51406\n",
            "epoch 178 train acc: 0.99457 dev_acc: 0.51458\n",
            "epoch 179 train acc: 0.99457 dev_acc: 0.51458\n",
            "epoch 180 train acc: 0.99457 dev_acc: 0.51458\n",
            "epoch 181 train acc: 0.99457 dev_acc: 0.51458\n",
            "epoch 182 train acc: 0.99457 dev_acc: 0.51354\n",
            "epoch 183 train acc: 0.99457 dev_acc: 0.51354\n",
            "epoch 184 train acc: 0.99457 dev_acc: 0.51354\n",
            "epoch 185 train acc: 0.99457 dev_acc: 0.51302\n",
            "epoch 186 train acc: 0.99457 dev_acc: 0.5125\n",
            "epoch 187 train acc: 0.99457 dev_acc: 0.51302\n",
            "epoch 188 train acc: 0.99457 dev_acc: 0.51302\n",
            "epoch 189 train acc: 0.99457 dev_acc: 0.5125\n",
            "epoch 190 train acc: 0.99457 dev_acc: 0.51198\n",
            "epoch 191 train acc: 0.99457 dev_acc: 0.51198\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-176-3daf28795ade>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopti\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"UNIK_best_optim.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"epoch\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"train acc:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrect\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"dev_acc:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-176-3daf28795ade>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mopti\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mlogit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-162-aecf3a2dedac>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-162-aecf3a2dedac>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt_unit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms_unit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresidual\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-162-aecf3a2dedac>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0mA1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mA1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mA2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m             \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_d\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mz\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1043\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1045\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1046\u001b[0m         \u001b[0mforward_call\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m         \u001b[0;31m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35vWxYnGxncv",
        "outputId": "cc41e06f-fd42-49b5-9e67-cdc12e58df0a"
      },
      "source": [
        "def predict(net,dataloader):\n",
        "    net.eval()\n",
        "    predictions = torch.tensor([]).cuda(gpu)\n",
        "    with torch.no_grad():\n",
        "        for i, (x,y) in enumerate(test_loader):\n",
        "            x,y=x.cuda(gpu),y.cuda(gpu)\n",
        "            logit = net(x)\n",
        "            pred = torch.argmax(logit,dim=1)+1\n",
        "            predictions=torch.cat((predictions,pred))\n",
        "    return predictions.cpu().numpy()\n",
        "pred = predict(net,test_loader)\n",
        "print(pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[22. 11.  9. ... 32. 13. 30.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tiSrPYEvx2YX"
      },
      "source": [
        "output = pd.read_csv(root_path+\"sample.csv\")\n",
        "output['Category']=pred.astype('int')\n",
        "output.head()\n",
        "output.to_csv(\"predictions.csv\",index=None)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}